{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, r2_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import glob\n",
    "\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = '/home/jupyter/ADAPT_PCR_share/safe/dataset'\n",
    "!ls $DATAPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('%s/0716_dataset_train.csv'%DATAPATH,index_col=[0,1])\n",
    "val_df = pd.read_csv('%s/0716_dataset_valid.csv'%DATAPATH,index_col=[0,1])\n",
    "test_df = pd.read_csv('%s/0716_dataset_test.csv'%DATAPATH,index_col=[0,1])\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "print(test_df.shape)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None) # Adjust display width for long lines\n",
    "pd.set_option('display.max_colwidth', None) # Display full content of cells\n",
    "\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = train_df.columns\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_df['prod_Tm'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handcrafted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = train_df[[\"f_length\", \"f_indel\",\"f_mm\",\"r_length\", \"r_indel\",\"r_mm\",\"f_Tm\", \"r_Tm\", \"prod_length\", \"prod_Tm\"]]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(inputs)\n",
    "X_train_scaled = scaler.transform(inputs)\n",
    "X_val_scaled = scaler.transform(val_df[[\"f_length\", \"f_indel\",\"f_mm\",\"r_length\", \"r_indel\",\"r_mm\",\"f_Tm\", \"r_Tm\", \"prod_length\", \"prod_Tm\"]])\n",
    "X_test_scaled = scaler.transform(test_df[[\"f_length\", \"f_indel\",\"f_mm\",\"r_length\", \"r_indel\",\"r_mm\",\"f_Tm\", \"r_Tm\", \"prod_length\", \"prod_Tm\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-based classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCRDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.Tensor(np.array(X)) \n",
    "        self.y = torch.Tensor(np.array(y))\n",
    "        self.len=len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXPLEN = max(train_df['f_penc'].apply(len).max(), train_df['r_penc'].apply(len).max())\n",
    "print(MAXPLEN)\n",
    "\n",
    "def one_hot_encode(seq, length=28):\n",
    "    mapping = { 'A':[1, 0, 0, 0, 0],\n",
    "                'T':[0, 1, 0, 0, 0],\n",
    "                'C':[0, 0, 1, 0, 0],\n",
    "                'G':[0, 0, 0, 1, 0],\n",
    "                'N':[0, 0, 0, 0, 0],\n",
    "                '-':[0, 0, 0, 0, 1] }\n",
    "    seq = seq.ljust(length, 'N') # (6, ATCG) -> NNATCG\n",
    "    return np.array([mapping[char.upper()] for char in seq])\n",
    "\n",
    "def one_hot_encode_full_gap(df_seqs, maxl=1421):\n",
    "    primer_encoded = []\n",
    "    target_encoded = []\n",
    "    for (tname,pname),row in df_seqs.iterrows():\n",
    "        fseq, fst, rseq, rst, tseq = row[['f_seq','f_start','r_seq','r_start','target_seq']]\n",
    "        fenc, ftenc, renc, rtenc = row[['f_penc','f_tenc','r_penc','r_tenc']]\n",
    "        pseq = 'N'*fst + fenc + 'N'*(rst-(fst+len(fseq))) + renc + 'N'*(len(tseq)-(rst+len(rseq)))\n",
    "        tseq = tseq[:fst] + ftenc + tseq[fst+len(fseq):rst] + rtenc + tseq[rst+len(rseq):]\n",
    "        primer_encoded.append(one_hot_encode(pseq, maxl))\n",
    "        target_encoded.append(one_hot_encode(tseq, maxl))\n",
    "    final_encoded = np.append(np.array(target_encoded), np.array(primer_encoded), axis=2)\n",
    "    print(final_encoded.shape)\n",
    "    return torch.tensor(final_encoded, dtype=torch.float32)\n",
    "def one_hot_encode_sequences(df_seqs):\n",
    "    primer_encoded = []\n",
    "    target_encoded = []\n",
    "    for (tname,pname),row in df_seqs.iterrows():\n",
    "        fenc, ftenc, renc, rtenc = row[['f_penc','f_tenc','r_penc','r_tenc']].apply(one_hot_encode)\n",
    "        prienc = np.append(fenc,renc,axis=0)\n",
    "        tarenc = np.append(ftenc,rtenc,axis=0)\n",
    "        primer_encoded.append(prienc)\n",
    "        target_encoded.append(tarenc)\n",
    "    primer_encoded = np.array(primer_encoded)\n",
    "    target_encoded = np.array(target_encoded)\n",
    "    final_encoded = np.append(target_encoded, primer_encoded, axis=2)\n",
    "    print(final_encoded.shape)\n",
    "    return torch.tensor(final_encoded, dtype=torch.float32)\n",
    "\n",
    "class PCRDataset(Dataset):\n",
    "    def __init__(self, encoded_input, custom_features, ct_values):\n",
    "        \"\"\"\n",
    "        encoded_input: consists of a tensor containing (8 x 56 encoding of the sequence).\n",
    "                       the upper 4 rows are the one-hot encoding of the target seq, \n",
    "                       the lower 4 rows are the one-hot encoding of the primer seq.\n",
    "        ct_values: a tensor containing the ct values for each primer pair and target sequence\n",
    "        \"\"\"\n",
    "        self.encoded_input = encoded_input\n",
    "        self.custom_features = custom_features\n",
    "        self.ct_values = ct_values\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_input)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_input[idx], self.custom_features[idx], self.ct_values[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change between these for either full-sequence or core sequence testing\n",
    "\n",
    "# train_inps = one_hot_encode_sequences(train_df)\n",
    "# val_inps = one_hot_encode_sequences(val_df)\n",
    "# test_inps = one_hot_encode_sequences(test_df)\n",
    "\n",
    "# train_inps = one_hot_encode_full_gap(train_df)\n",
    "# val_inps = one_hot_encode_full_gap(val_df)\n",
    "# test_inps = one_hot_encode_full_gap(test_df)\n",
    "# # useful for cross-validation\n",
    "# train_and_val_inps = torch.cat((train_inps,val_inps))\n",
    "\n",
    "cutoff=0\n",
    "\n",
    "train_labels = torch.tensor(np.where(train_df['score'] > cutoff, 1, 0))\n",
    "val_labels = torch.tensor(np.where(val_df['score'] > cutoff, 1, 0))\n",
    "test_labels = torch.tensor(np.where(test_df['score'] > cutoff, 1, 0))\n",
    "train_val_labels = torch.cat((train_labels, val_labels))\n",
    "\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = PCRDataset(train_inps, torch.tensor(X_train_scaled), train_labels)\n",
    "val_dataset = PCRDataset(val_inps, torch.tensor(X_val_scaled), val_labels)\n",
    "test_dataset = PCRDataset(test_inps, torch.tensor(X_test_scaled), test_labels)\n",
    "train_val_dataset = PCRDataset(train_and_val_inps, torch.tensor(np.concatenate([X_train_scaled, X_val_scaled])), train_val_labels)\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) # can play around\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "train_val_dataset = DataLoader(train_val_dataset, batch_size=64, shuffle=False) # dont want it interfering with cross-val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import dump, load\n",
    "from tqdm.auto import tqdm\n",
    "from Bio import Seq\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGC(nn.Module):\n",
    "    def __init__(self,d_model,expansion_factor = 1.0,dropout = 0.0):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.dropout = dropout\n",
    "        self.conv = nn.Conv1d(int(d_model * expansion_factor), int(d_model * expansion_factor),\n",
    "                              kernel_size=3, padding=1, groups=int(d_model * expansion_factor))\n",
    "        self.in_proj = nn.Linear(d_model, int(d_model * expansion_factor * 2))\n",
    "        self.out_norm = nn.RMSNorm(int(d_model), eps=1e-8)\n",
    "        self.in_norm = nn.RMSNorm(int(d_model * expansion_factor * 2), eps=1e-8)\n",
    "        self.out_proj = nn.Linear(int(d_model * expansion_factor), d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, u):\n",
    "        xv = self.in_norm(self.in_proj(u))\n",
    "        x,v = xv.chunk(2,dim=-1)\n",
    "        x_conv = self.conv(x.transpose(-1,-2)).transpose(-1,-2)\n",
    "        gate =  v * x_conv\n",
    "        x = self.out_norm(self.out_proj(gate))\n",
    "        return x\n",
    "    \n",
    "class DropoutNd(nn.Module):\n",
    "    def __init__(self, p: float = 0.5, tie=True, transposed=True):\n",
    "        \"\"\"\n",
    "        tie: tie dropout mask across sequence lengths (Dropout1d/2d/3d)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if p < 0 or p >= 1:\n",
    "            raise ValueError(\"dropout probability has to be in [0, 1), \" \"but got {}\".format(p))\n",
    "        self.p = p\n",
    "        self.tie = tie\n",
    "        self.transposed = transposed\n",
    "        self.binomial = torch.distributions.binomial.Binomial(probs=1-self.p)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"X: (batch, dim, lengths...).\"\"\"\n",
    "        if self.training:\n",
    "            if not self.transposed: X = rearrange(X, 'b ... d -> b d ...')\n",
    "            # binomial = torch.distributions.binomial.Binomial(probs=1-self.p) \n",
    "            # This is incredibly slow because of CPU -> GPU copying\n",
    "            mask_shape = X.shape[:2] + (1,)*(X.ndim-2) if self.tie else X.shape\n",
    "            # mask = self.binomial.sample(mask_shape)\n",
    "            mask = torch.rand(*mask_shape, device=X.device) < 1.-self.p\n",
    "            X = X * mask * (1.0/(1-self.p))\n",
    "            if not self.transposed: X = rearrange(X, 'b d ... -> b ... d')\n",
    "            return X\n",
    "        return X\n",
    "\n",
    "class S4DKernel(nn.Module):\n",
    "    \"\"\"Generate convolution kernel from diagonal SSM parameters.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, N=64, dt_min=0.001, dt_max=0.1, lr=None):\n",
    "        super().__init__()\n",
    "        # Generate dt\n",
    "        H = d_model\n",
    "        log_dt = torch.rand(H) * (\n",
    "            math.log(dt_max) - math.log(dt_min)\n",
    "        ) + math.log(dt_min)\n",
    "\n",
    "        C = torch.randn(H, N // 2, dtype=torch.cfloat)\n",
    "        self.C = nn.Parameter(torch.view_as_real(C))\n",
    "        self.register(\"log_dt\", log_dt, lr)\n",
    "\n",
    "        log_A_real = torch.log(0.5 * torch.ones(H, N//2))\n",
    "        A_imag = math.pi * repeat(torch.arange(N//2), 'n -> h n', h=H)\n",
    "        self.register(\"log_A_real\", log_A_real, lr)\n",
    "        self.register(\"A_imag\", A_imag, lr)\n",
    "\n",
    "    def forward(self, L):\n",
    "        \"\"\"\n",
    "        returns: (..., c, L) where c is number of channels (default 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # Materialize parameters\n",
    "        dt = torch.exp(self.log_dt) # (H)\n",
    "        C = torch.view_as_complex(self.C) # (H N)\n",
    "        A = -torch.exp(self.log_A_real) + 1j * self.A_imag # (H N)\n",
    "\n",
    "        # Vandermonde multiplication\n",
    "        dtA = A * dt.unsqueeze(-1)  # (H N)\n",
    "        K = dtA.unsqueeze(-1) * torch.arange(L, device=A.device) # (H N L)\n",
    "        C = C * (torch.exp(dtA)-1.) / A\n",
    "        K = 2 * torch.einsum('hn, hnl -> hl', C, torch.exp(K)).real\n",
    "\n",
    "        return K\n",
    "\n",
    "    def register(self, name, tensor, lr=None):\n",
    "        \"\"\"Register a tensor with a configurable learning rate and 0 weight decay\"\"\"\n",
    "\n",
    "        if lr == 0.0:\n",
    "            self.register_buffer(name, tensor)\n",
    "        else:\n",
    "            self.register_parameter(name, nn.Parameter(tensor))\n",
    "\n",
    "            optim = {\"weight_decay\": 0.0}\n",
    "            if lr is not None: optim[\"lr\"] = lr\n",
    "            setattr(getattr(self, name), \"_optim\", optim)\n",
    "\n",
    "\n",
    "class S4D(nn.Module):\n",
    "    def __init__(self, d_model, d_state=64, dropout=0.0, transposed=True, **kernel_args):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h = d_model\n",
    "        self.n = d_state\n",
    "        self.d_output = self.h\n",
    "        self.transposed = transposed\n",
    "\n",
    "        self.D = nn.Parameter(torch.randn(self.h))\n",
    "        # SSM Kernel\n",
    "        self.kernel = S4DKernel(self.h, N=self.n, **kernel_args)\n",
    "        # Pointwise\n",
    "        self.activation = nn.GELU()\n",
    "        dropout_fn = DropoutNd\n",
    "        self.dropout = dropout_fn(dropout) if dropout > 0.0 else nn.Identity()\n",
    "\n",
    "        # position-wise output transform to mix features\n",
    "        self.output_linear = nn.Sequential(\n",
    "            nn.Conv1d(self.h, 2*self.h, kernel_size=1),\n",
    "            nn.GLU(dim=-2),\n",
    "        )\n",
    "\n",
    "    def forward(self, u, **kwargs): # absorbs return_output and transformer src mask\n",
    "        \"\"\" Input and output shape (B, H, L) \"\"\"\n",
    "        if not self.transposed: u = u.transpose(-1, -2)\n",
    "        L = u.size(-1)\n",
    "        # Compute SSM Kernel\n",
    "        k = self.kernel(L=L) # (H L)\n",
    "\n",
    "        # Convolution\n",
    "        k_f = torch.fft.rfft(k, n=2*L)  # (H L)\n",
    "        u_f = torch.fft.rfft(u, n=2*L) # (B H L)\n",
    "        y = torch.fft.irfft(u_f*k_f, n=2*L)[..., :L] # (B H L)\n",
    "\n",
    "        # Compute D term in state space equation - essentially a skip connection\n",
    "        y = y + u * self.D.unsqueeze(-1)\n",
    "\n",
    "        y = self.dropout(self.activation(y))\n",
    "        y = self.output_linear(y)\n",
    "        if not self.transposed: y = y.transpose(-1, -2)\n",
    "        return y\n",
    "    \n",
    "class Janus(nn.Module):\n",
    "    def __init__(self, d_input, d_output, d_model, d_state=64, dropout=0.2, transposed=False, **kernel_args):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(d_input, d_model)\n",
    "        self.pgc1 = PGC(d_model, expansion_factor=0.25, dropout=dropout)\n",
    "        self.pgc2 = PGC(d_model, expansion_factor=2, dropout=dropout)\n",
    "        self.s4d = S4D(d_model, d_state=d_state, dropout=dropout, transposed=transposed, **kernel_args)\n",
    "        self.norm = nn.RMSNorm(d_model)\n",
    "        self.decoder = nn.Linear(d_model, d_output)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, u):\n",
    "        x = self.encoder(u)\n",
    "        x = self.pgc1(x)\n",
    "        x = self.pgc2(x)\n",
    "        z = x\n",
    "        z = self.norm(z)\n",
    "        x = self.dropout(self.s4d(z)) + x\n",
    "        x = x.mean(dim=1)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, mlp_dims, ssm_dims, combined_hidden, final_output):\n",
    "        super(CombinedModel, self).__init__()\n",
    "\n",
    "        # Individual models\n",
    "        self.mlp = MLP(*mlp_dims)\n",
    "        self.ssm = Janus(*ssm_dims)\n",
    "\n",
    "        # Combining MLP\n",
    "        combined_input_dim = mlp_dims[-1] + ssm_dims[1]\n",
    "        self.combiner = nn.Sequential(\n",
    "            nn.Linear(combined_input_dim, combined_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(combined_hidden, final_output),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, mlp_input, ssm_input):\n",
    "        mlp_out = self.mlp(mlp_input)  # Output from MLP\n",
    "        ssm_out = self.ssm(ssm_input)  # Output from SSM\n",
    "\n",
    "        # Concatenate outputs\n",
    "        combined = torch.cat((mlp_out, ssm_out), dim=1)\n",
    "\n",
    "        # Final prediction\n",
    "        final_output = self.combiner(combined)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def train_loop(model, lr):\n",
    "    # Initialize lists to store performance metrics for different data volumes\n",
    "    metrics = []\n",
    "    num_epochs = 50  # Adjust\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Train Janus model\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01) #lr = 0.001\n",
    "    best_cross_loss = float('inf')\n",
    "    best_r2 = float('-inf') \n",
    "    best_model_state = None\n",
    "    \n",
    "    # setup the k-fold cross validation\n",
    "    k_folds = 5\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    trues, preds, tprs, fprs = {}, {}, {}, {}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_true = []\n",
    "        train_pred = []\n",
    "\n",
    "\n",
    "        for inputs, mlp_inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Janus Training'):\n",
    "            inputs, mlp_inputs, labels = inputs.to(device).float(), mlp_inputs.to(device).float(), labels.to(device).float()\n",
    "            outputs = model(mlp_inputs, inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_true.append(labels.detach().cpu().numpy())\n",
    "            outputs = (outputs >= 0.5).int()\n",
    "            train_pred.append(outputs.squeeze().detach().cpu().numpy())\n",
    "\n",
    "        # Evaluate on cross-validation set\n",
    "        model.eval()\n",
    "        cross_loss = 0\n",
    "        cross_true = []\n",
    "        cross_pred = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for inputs, mlp_inputs, labels in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Cross Validation'):\n",
    "                inputs, mlp_inputs, labels = inputs.to(device).float(), mlp_inputs.to(device).float(), labels.to(device).float()\n",
    "                outputs = model(mlp_inputs, inputs)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                cross_loss += loss.item()\n",
    "                cross_true.append(labels.detach().cpu().numpy())\n",
    "                outputs = (outputs >= 0.5).int()\n",
    "                cross_pred.append(outputs.squeeze().detach().cpu().numpy())\n",
    "\n",
    "            cross_true = np.concatenate(cross_true)\n",
    "            cross_pred = np.concatenate(cross_pred)\n",
    "\n",
    "        # Flatten the collected predictions and true labels\n",
    "        all_true = np.concatenate([cross_true])\n",
    "        all_pred = np.concatenate([cross_pred])\n",
    "\n",
    "        # Compute confusion matrix: [[TN, FP], [FN, TP]]\n",
    "        tn, fp, fn, tp = confusion_matrix(all_true, all_pred).ravel()\n",
    "\n",
    "        # Compute TPR and FPR\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "        # Print statistics\n",
    "        print(f\"\\nEpoch {epoch+1} Cross-validation Statistics:\")\n",
    "        print(f\"Cross-validation Loss: {cross_loss/len(val_loader):.4f}\")\n",
    "        print(f\"TPR (Recall): {tpr:.4f}\")\n",
    "        print(f\"FPR: {fpr:.4f}\")\n",
    "\n",
    "        # Save metrics\n",
    "        trues[epoch] = cross_true\n",
    "        preds[epoch] = cross_pred\n",
    "        tprs[epoch] = tpr\n",
    "        fprs[epoch] = fpr\n",
    "\n",
    "        # Save the best model\n",
    "        if tpr > best_tpr:  \n",
    "            best_tpr = tpr\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        if cross_loss < best_cross_loss:\n",
    "            best_cross_loss = cross_loss\n",
    "    \n",
    "    plt.title(f\"lr = {lr}\")\n",
    "    plt.plot(list(range(num_epochs)), list(tprs.values()))\n",
    "    plt.plot(list(range(num_epochs)), list(fprs.values()))\n",
    "    plt.legend([\"TPR\", \"FPR\"])\n",
    "    plt.show()\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def evaluate_auroc(model, data_loader, device, results_df=None, model_name=\"Model\"):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, mlp_inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device).float()\n",
    "            mlp_inputs = mlp_inputs.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(mlp_inputs, inputs)\n",
    "\n",
    "            preds = outputs.squeeze().cpu().numpy() # probability values for each in [0,1]\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    output = (np.array(all_preds) >= 0.5).astype(int) # thresholding to convert to 0s and 1s\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, output)\n",
    "    precision = precision_score(all_labels, output, zero_division=0)\n",
    "    recall = recall_score(all_labels, output, zero_division=0)\n",
    "    f1 = f1_score(all_labels, output, zero_division=0)\n",
    "    try:\n",
    "        auroc = roc_auc_score(all_labels, all_preds) # the only one we actually want preds from\n",
    "    except ValueError:\n",
    "        auroc = float('nan')  # in case only one class in y_true\n",
    "\n",
    "    # Prepare results\n",
    "    result_row = pd.DataFrame([{\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1,\n",
    "        'AUROC': auroc\n",
    "    }])\n",
    "\n",
    "    # Append to existing DataFrame\n",
    "    if results_df is None:\n",
    "        results_df = result_row\n",
    "    else:\n",
    "        results_df = pd.concat([results_df, result_row], ignore_index=True)\n",
    "\n",
    "    return all_labels, all_preds, results_df\n",
    "\n",
    "def plot_auroc(labels, preds):\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    fpr, tpr, _ = roc_curve(labels, preds)\n",
    "    print(auc)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'AUROC = {auc:.4f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Random guess line\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('AUROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "janus_inp_dim = 10\n",
    "janus_out_dim = 4\n",
    "janus_mod_dims = [64, 128]\n",
    "\n",
    "mlp_inp_dim = X_test_scaled.shape[1]\n",
    "mlp_hid_dims = [32, 64]\n",
    "mlp_out_dim = 4\n",
    "\n",
    "com_hid_dim = 32\n",
    "final_out_dim = 1\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "lrs = [0.001]\n",
    "\n",
    "for janus_mod_dim in janus_mod_dims:\n",
    "     for mlp_hid_dim in mlp_hid_dims:\n",
    "        for lr in lrs:\n",
    "            model = CombinedModel(mlp_dims=(mlp_inp_dim, mlp_hid_dim, mlp_out_dim), \n",
    "                  ssm_dims=(janus_inp_dim, janus_out_dim, janus_mod_dim), \n",
    "                  combined_hidden=com_hid_dim, final_output=final_out_dim).to(device)\n",
    "            train_loop(model, lr=lr)\n",
    "            labels, preds, results_df = evaluate_auroc(model, test_loader, device, results_df, model_name=f\"Combined, janus:{janus_mod_dim}, mlp:{mlp_hid_dim}, lr:{lr}\")\n",
    "            plot_auroc(labels, preds)\n",
    "\n",
    "            \n",
    "display(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
