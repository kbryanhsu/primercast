{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __init__ import *\n",
    "from adapt_pcr_external_tools import *\n",
    "from primer_modules import *\n",
    "from ml_modules import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "import gzip\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from Bio import SeqIO, Seq, Entrez\n",
    "from io import StringIO\n",
    "from Levenshtein import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.SeqUtils import MeltingTemp, gc_fraction\n",
    "def get_tm(seq):\n",
    "    return MeltingTemp.Tm_NN(seq, Na=50, Mg=1.5, dNTPs=.6)\n",
    "\n",
    "def get_dg_vienna(seq1, seq2):\n",
    "    inp = '\"%s\\n%s\"'%(seq1,seq2)\n",
    "    res = !echo -e $inp | $RNAduplex - --noconv --paramFile=DNA 2>tmp\n",
    "    dg = float(res[0].split()[-1][1:-1])\n",
    "    return dg\n",
    "\n",
    "Entrez.email = \"bsc0371@gmail.com\"\n",
    "def get_cdna_sequence(nm_id):\n",
    "    handle = Entrez.efetch(db=\"nucleotide\", id=nm_id, rettype=\"fasta\", retmode=\"text\")\n",
    "    fasta_data = handle.read()\n",
    "    handle.close()\n",
    "    \n",
    "    record = SeqIO.read(StringIO(fasta_data), \"fasta\")\n",
    "    return record.description, str(record.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML = '/home/jupyter/ADAPT_PCR_share/safe/resources/ml/0725_combined_model.pth'\n",
    "\n",
    "ML = '/home/jupyter/ADAPT_PCR_share/safe/resources/ml/0725_combined_model.pth'\n",
    "SCALER = '/home/jupyter/ADAPT_PCR_share/safe/resources/ml/0728_scaler.joblib'\n",
    "FEATS = ['f_len','f_Tm','f_GC','f_indel','f_mm','r_len','r_Tm','r_GC','r_indel','r_mm','prod_len','prod_Tm']\n",
    "\n",
    "scaler = joblib.load(SCALER)\n",
    "model = torch.load(ML, weights_only=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, bidirectional=False):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        # Fully connected layer to change shape into output size\n",
    "        self.fc = nn.Linear(hidden_size * self.num_directions, output_size)\n",
    "        self.sigout = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers * self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers * self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: (batch_size, seq_len, hidden_size * num_directions)\n",
    "\n",
    "        out = self.fc(out[:, -1, :])  # Last time step\n",
    "        out = self.sigout(out)\n",
    "        return out\n",
    "\n",
    "import pickle\n",
    "classifier_path = '/home/jupyter/ADAPT_PCR_share/safe/resources/ml/0721_LSTM_Classifier.pkl'\n",
    "classifier = pickle.load(open(classifier_path, 'rb'))\n",
    "classifier.lstm.flatten_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_pbs_gap(df_seqs):\n",
    "    primer_encoded = []\n",
    "    target_encoded = []\n",
    "    for i,row in df_seqs.iterrows():\n",
    "        fenc, ftenc, renc, rtenc = row[['pseq_f','tseq_f','pseq_r','tseq_r']].apply(one_hot_encode)\n",
    "        prienc = np.append(fenc,renc,axis=0)\n",
    "        tarenc = np.append(ftenc,rtenc,axis=0)\n",
    "        primer_encoded.append(prienc)\n",
    "        target_encoded.append(tarenc)\n",
    "    primer_encoded = np.array(primer_encoded)\n",
    "    target_encoded = np.array(target_encoded)\n",
    "    final_encoded = np.append(target_encoded, primer_encoded, axis=2)\n",
    "    print(final_encoded.shape)\n",
    "    return torch.tensor(final_encoded, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genef1 = \"/home/jupyter/ADAPT_PCR_share/safe/resources/genomes/human_all.rna.fna.gz\"\n",
    "# genef2 = \"/home/jupyter/ADAPT_PCR_share/safe/resources/genomes/refMrna.fa.gz\"\n",
    "# nmseqs = { s.id.split('.')[0]:str(s.seq) for s in SeqIO.parse(gzip.open(genef1,'rt'), \"fasta\") }\n",
    "# print('# NMs in f1:', len(nmseqs))\n",
    "# for s in SeqIO.parse(gzip.open(genef2,'rt'), \"fasta\"):\n",
    "#     try:\n",
    "#         seq = nmseqs[s.id]\n",
    "#     except KeyError:\n",
    "#         nmseqs[s.id] = str(s.seq).upper()\n",
    "# print('# NMs in f1 & f2:', len(nmseqs))\n",
    "\n",
    "# prinms = set(oritbl['NM'].tolist())\n",
    "# tseqs = {}\n",
    "# for nm in prinms:\n",
    "#     if nm not in allnms:\n",
    "#         tseqs[nm] = get_cdna_sequence(nm)[1]\n",
    "\n",
    "# genef3 = \"/home/jupyter/ADAPT_PCR_share/safe/resources/genomes/Origene_missing.fa\"\n",
    "# with open(genef3, 'wt') as out:\n",
    "#     for nm in sorted(tseqs.keys()):\n",
    "#         out.write('>%s\\n%s\\n' % (nm, tseqs[nm].upper()))\n",
    "\n",
    "# genef3 = \"/home/jupyter/ADAPT_PCR_share/safe/resources/genomes/Origene_missing.fa\"\n",
    "# for s in SeqIO.parse(genef3, \"fasta\"):\n",
    "#     nmseqs[s.id] = str(s.seq)\n",
    "# print('# NMs in f1, f2 & f3:', len(nmseqs))\n",
    "\n",
    "genef_comb = \"/home/jupyter/ADAPT_PCR_share/safe/resources/genomes/nm_combined.fa\"\n",
    "# with open(genef_comb, 'wt') as out:\n",
    "#     for nm in sorted(nmseqs.keys()):\n",
    "#         out.write('>%s\\n%s\\n' % (nm, nmseqs[nm].upper()))\n",
    "\n",
    "nmseqs = {}\n",
    "for s in SeqIO.parse(genef_comb, \"fasta\"):\n",
    "    nmseqs[s.id] = str(s.seq)\n",
    "print('# NMs:', len(nmseqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Origene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allnms = set(nmseqs.keys())\n",
    "pseqf = '/home/jupyter/ADAPT_PCR_share/safe/resources/origene_primers_0731.csv'\n",
    "tmptbl = pd.read_csv(pseqf).drop_duplicates('NM').dropna()\n",
    "oritbl = tmptbl[tmptbl['NM'].apply(lambda nm: nm.startswith('NM') and nm in allnms)].drop_duplicates('NM')\n",
    "print('# primers:', len(tmptbl))\n",
    "print('# target seq prepared:', len(oritbl))\n",
    "oritbl.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "cols = ['Gene', 'tname', 'pseq_f', 'pseq_r']\n",
    "def find_match(seq, ref, dcut=5):\n",
    "    if len(ref) < len(seq):\n",
    "        return (-1, 999) \n",
    "    if seq in ref:\n",
    "        return (ref.find(seq), 0)\n",
    "    sortvs = sorted([(distance(seq, ref[i:i+len(seq)]), i) for i in range(len(ref)-len(seq))])\n",
    "    d, st = sortvs[0]\n",
    "    return (st, d) if d <= dcut else (-1, 999)\n",
    "\n",
    "def prepare_input_parallel(df, nmseqs, parse_func):\n",
    "    rows = []\n",
    "    for row in df.to_dict('records'):\n",
    "        row['tseq'] = nmseqs[row['tname']]\n",
    "        rows.append(row)\n",
    "        \n",
    "    with Pool(processes=cpu_count()) as pool:\n",
    "        results = tqdm(pool.map(parse_func, rows), total=len(rows))\n",
    "\n",
    "    filtered_rows = [row for row in results if row is not None]\n",
    "    return pd.DataFrame(filtered_rows)\n",
    "\n",
    "def prepare_input(row):\n",
    "    gene, nm, fseq, rseq, tseq = row['Gene'], row['tname'], row['pseq_f'], row['pseq_r'], row['tseq']\n",
    "    \n",
    "    fst, fd = find_match(fseq, tseq)\n",
    "    rst, rd = find_match(reverse_complement_dna(rseq), tseq[fst:])\n",
    "    rst = fst + rst\n",
    "    \n",
    "    if 0 <= fst < rst:\n",
    "        row['tseq_f'] = tseq[fst:fst + len(fseq)]\n",
    "        row['tseq_r'] = reverse_complement_dna(tseq[rst:rst + len(rseq)])\n",
    "        prod = tseq[fst:rst + len(rseq)]\n",
    "        row['prod_len'] = len(prod)\n",
    "        row['prod_Tm'] = get_tm(prod)\n",
    "        \n",
    "        for prefix, pseq, dist in zip(['f', 'r'], [fseq, rseq], [fd, rd]):\n",
    "            row[f'{prefix}_len'] = len(pseq)\n",
    "            row[f'{prefix}_GC'] = gc_fraction(pseq)\n",
    "            row[f'{prefix}_Tm'] = get_tm(pseq)\n",
    "            row[f'{prefix}_indel'] = 0\n",
    "            row[f'{prefix}_mm'] = dist\n",
    "\n",
    "        return row\n",
    "    return None\n",
    "\n",
    "def mut_nuc(seq):\n",
    "    mutd = {'A':'G','G':'A','T':'C','C':'T'}\n",
    "    return ''.join([mutd[nuc] for nuc in seq])\n",
    "\n",
    "def introduce_5p_mut(seq, mutl):\n",
    "    #return mut_nuc(seq[:mutl]) + seq[mutl:]\n",
    "    return reverse_complement_dna(seq[:mutl]) + seq[mutl:]\n",
    "\n",
    "def introduce_3p_mut(seq, mutl):\n",
    "    #return mut_nuc(seq[:mutl]) + seq[mutl:]\n",
    "    return seq[:-mutl] + reverse_complement_dna(seq[-mutl:])\n",
    "\n",
    "def prepare_input_5p_mut(row, mutcnts=range(1,9), weights=[1,2,3,4,4,3,2,1]):\n",
    "    gene, nm, fseq, rseq, tseq = row['Gene'], row['tname'], row['pseq_f'], row['pseq_r'], row['tseq']\n",
    "    \n",
    "    fst, fd = find_match(fseq, tseq)\n",
    "    rst, rd = find_match(reverse_complement_dna(rseq), tseq[fst:])\n",
    "    rst = fst + rst\n",
    "        \n",
    "    if 0 <= fst < rst:\n",
    "        fmut = random.choices(mutcnts, weights=weights, k=1)[0]\n",
    "        rmut = random.choices(mutcnts, weights=weights, k=1)[0]\n",
    "        row['tseq_f'] = introduce_5p_mut(tseq[fst:fst + len(fseq)], fmut)\n",
    "        row['tseq_r'] = introduce_5p_mut(reverse_complement_dna(tseq[rst:rst + len(rseq)]), rmut)\n",
    "        prod = tseq[fst:rst + len(rseq)]\n",
    "        row['prod_len'] = len(prod)\n",
    "        row['prod_Tm'] = get_tm(prod)\n",
    "    \n",
    "        for prefix, pseq, dist, mcnt in zip(['f', 'r'], [fseq, rseq], [fd, rd], [fmut, rmut]):\n",
    "            row[f'{prefix}_len'] = len(pseq)\n",
    "            row[f'{prefix}_GC'] = gc_fraction(pseq)\n",
    "            row[f'{prefix}_Tm'] = get_tm(pseq)\n",
    "            row[f'{prefix}_indel'] = 0\n",
    "            row[f'{prefix}_mm'] = dist + mcnt\n",
    "\n",
    "        return row\n",
    "    return None\n",
    "\n",
    "def prepare_input_3p_mut(row, mutcnts=range(3,6), weights=[1,1,1]):\n",
    "    gene, nm, fseq, rseq, tseq = row['Gene'], row['tname'], row['pseq_f'], row['pseq_r'], row['tseq']\n",
    "    \n",
    "    fst, fd = find_match(fseq, tseq)\n",
    "    rst, rd = find_match(reverse_complement_dna(rseq), tseq[fst:])\n",
    "    rst = fst + rst\n",
    "        \n",
    "    if 0 <= fst < rst:\n",
    "        fmut = random.choices(mutcnts, weights=weights, k=1)[0]\n",
    "        rmut = random.choices(mutcnts, weights=weights, k=1)[0]\n",
    "        row['tseq_f'] = introduce_3p_mut(tseq[fst:fst + len(fseq)], fmut)\n",
    "        row['tseq_r'] = introduce_3p_mut(reverse_complement_dna(tseq[rst:rst + len(rseq)]), rmut)\n",
    "        prod = tseq[fst:rst + len(rseq)]\n",
    "        row['prod_len'] = len(prod)\n",
    "        row['prod_Tm'] = get_tm(prod)\n",
    "    \n",
    "        for prefix, pseq, dist, mcnt in zip(['f', 'r'], [fseq, rseq], [fd, rd], [fmut, rmut]):\n",
    "            row[f'{prefix}_len'] = len(pseq)\n",
    "            row[f'{prefix}_GC'] = gc_fraction(pseq)\n",
    "            row[f'{prefix}_Tm'] = get_tm(pseq)\n",
    "            row[f'{prefix}_indel'] = 0\n",
    "            row[f'{prefix}_mm'] = dist + mcnt\n",
    "\n",
    "        return row\n",
    "    return None\n",
    "\n",
    "start = time.time()\n",
    "inp = oritbl.drop(['URL'], axis=1)\n",
    "inp.columns = cols\n",
    "inptbl = prepare_input_parallel(inp, nmseqs, prepare_input).dropna()\n",
    "inptbl_5pmut = prepare_input_parallel(inp, nmseqs, prepare_input_5p_mut).dropna()\n",
    "inptbl_3pmut = prepare_input_parallel(inp, nmseqs, prepare_input_3p_mut).dropna()\n",
    "inptbl = pd.concat([inptbl,inptbl_5pmut])\n",
    "runtime = time.time() - start\n",
    "print('# valid inputs:', len(inptbl))\n",
    "print('Run time (parsing): %.3f sec' % runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_row(row):\n",
    "    fenc = one_hot_encode(row['pseq_f'])\n",
    "    ftenc = one_hot_encode(row['tseq_f'])\n",
    "    renc = one_hot_encode(row['pseq_r'])\n",
    "    rtenc = one_hot_encode(row['tseq_r'])\n",
    "\n",
    "    prienc = np.append(fenc, renc, axis=0)   # Primer\n",
    "    tarenc = np.append(ftenc, rtenc, axis=0) # Target\n",
    "    combined = np.append(tarenc, prienc, axis=1)\n",
    "    return combined\n",
    "\n",
    "def one_hot_encode_pbs_gap_parallel(df_seqs):\n",
    "    rows = df_seqs.to_dict('records')\n",
    "    with Pool(processes=cpu_count()) as pool:\n",
    "        results = pool.map(encode_row, rows)\n",
    "\n",
    "    final_encoded = np.array(results)  # (batch, 56, 10)\n",
    "    print(final_encoded.shape)\n",
    "    return torch.tensor(final_encoded, dtype=torch.float32)\n",
    "\n",
    "start = time.time()\n",
    "seq_input = one_hot_encode_pbs_gap_parallel(inptbl)\n",
    "fea_input = scaler.transform(inptbl[FEATS])\n",
    "labels = np.array([0]*len(inptbl))\n",
    "dataset = PcrDataset(seq_input, fea_input, labels)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "runtime = time.time() - start\n",
    "print('Run time (encoding): %.3f sec' % runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "inpf = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/origene_primers_0806.false.csv'\n",
    "\n",
    "start = time.time()\n",
    "predict_cls, predict_reg = [], []\n",
    "with torch.no_grad():\n",
    "    for seq_in, fea_in, lab in tqdm(loader, desc='Infering'):\n",
    "        seq_in, fea_in, lab = seq_in.to(device).float(), fea_in.to(device).float(), lab.to(device).float()\n",
    "        out_cls = classifier(seq_in)\n",
    "        out_reg = model(fea_in, seq_in)\n",
    "        if len(seq_in)==1:\n",
    "            predict_cls.append(np.array([out_cls.squeeze().detach().cpu().numpy()]))\n",
    "            predict_reg.append(np.array([out_reg.squeeze().detach().cpu().numpy()]))\n",
    "        else:\n",
    "            predict_cls.append(out_cls.squeeze().detach().cpu().numpy())\n",
    "            predict_reg.append(out_reg.squeeze().detach().cpu().numpy())\n",
    "    predict_cls = np.concatenate(predict_cls)\n",
    "    predict_reg = np.concatenate(predict_reg)\n",
    "inptbl.loc[:,'classifier'] = predict_cls\n",
    "inptbl.loc[:,'regressor'] = predict_reg\n",
    "inptbl.to_csv(inpf, index=False)\n",
    "runtime = time.time() - start\n",
    "print('Run time (inference): %.3f sec' % runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(5,2), sharey=True)\n",
    "ax1.hist(inptbl['classifier'])\n",
    "ax2.hist(inptbl['regressor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta = '/home/jupyter/ADAPT_PCR_share/safe/resources/origene_primers_0731.fa'\n",
    "# with open(fasta, 'wt') as out:\n",
    "#     for _, row in inptbl.iterrows():\n",
    "#         nm = row['tname']\n",
    "#         fseq = row['pseq_f']\n",
    "#         rseq = row['pseq_r']\n",
    "#         out.write('>%s-f\\n%s\\n>%s-r\\n%s\\n'%(nm, fseq, nm, rseq))\n",
    "!wc -l $fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping to viral sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viralf = '/home/jupyter/ADAPT_PCR_share/safe/resources/genomes/human_viral_coding.fa'\n",
    "viralseqs = {s.id:str(s.seq) for s in SeqIO.parse(viralf, 'fasta')}\n",
    "print('# viral coding seqs:', len(viralseqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapopt = '--mp 2,2 --rdg 4,4 --rfg 4,4 -L 8 -N 1 --score-min L,-0.6,-0.6'\n",
    "threads = 4\n",
    "ref = '/home/jupyter/ADAPT_PCR_share/safe/resources/genomes/bowtie2/human_viral_coding'\n",
    "sam = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/origene_primers_0731.viral.sam'\n",
    "multimap = 1000\n",
    "command = '%s -x %s -U %s -f -p %i -k %i %s --no-hd --no-unal > %s'\\\n",
    "        % (bowtie2, ref, fasta, threads, multimap, mapopt, sam)\n",
    "#!$command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = sam.replace('.sam','.parsed')\n",
    "pseqs = parsed + '.pseq'\n",
    "tseqs = parsed + '.tseq'\n",
    "mapped = sam.replace('.sam','.mapped')\n",
    "\n",
    "comm1 = '%s < %s > %s' % (sam2pairwise, sam, parsed)\n",
    "comm2 = 'awk \"NR %% 4 == 2\" %s > %s' % (parsed, pseqs)\n",
    "comm3 = 'awk \"NR %% 4 == 0\" %s > %s' % (parsed, tseqs)\n",
    "comm4 = 'cut -f1-4 %s | paste - %s | paste - %s > %s' % (sam, pseqs, tseqs, mapped)\n",
    "\n",
    "# !$comm1\n",
    "# !$comm2\n",
    "# !$comm3\n",
    "# !$comm4\n",
    "# delete([parsed,pseqs,tseqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['pname','orientation','tname','start','pseq','tseq']\n",
    "subcols = ['pname','start','pseq','tseq','ptarget']\n",
    "minl, maxl = 50, 10000\n",
    "def pair_primers(args):\n",
    "    tname, grp, tseq = args\n",
    "    #fors = grp.loc[(grp['orientation']==0)&(grp['forrev']=='f'), subcols]\n",
    "    #revs = grp.loc[(grp['orientation']==16)&(grp['forrev']=='r'), subcols]\n",
    "    fors = grp.loc[(grp['orientation']==0), subcols]\n",
    "    revs = grp.loc[(grp['orientation']==16), subcols]\n",
    "    revs['pseq'] = revs['pseq'].apply(rev_com_enc)\n",
    "    revs['tseq'] = revs['tseq'].apply(rev_com_enc)\n",
    "    if fors.empty or revs.empty:\n",
    "        return None\n",
    "    \n",
    "    paired = fors.merge(revs, on='ptarget', suffixes=('_f','_r'))\n",
    "    if paired.empty:\n",
    "        return None\n",
    "    \n",
    "    paired.loc[:, 'tname'] = tname\n",
    "    paired.loc[:, 'prod_len'] = paired['start_r'] - paired['start_f'] + paired['pseq_r'].apply(len) + 1\n",
    "    paired = paired[paired['prod_len'].apply(lambda x: minl <= x <= maxl)]\n",
    "    if paired.empty:\n",
    "        return None\n",
    "    \n",
    "    paired.loc[:, 'prod_Tm'] = paired.apply(lambda row:\n",
    "        get_tm(tseq[row['start_f']-1: row['start_r']+len(row['pseq_r'])]), axis=1)\n",
    "\n",
    "    paired.loc[:, 'f_len'] = paired['pseq_f'].apply(len)\n",
    "    paired.loc[:, 'f_GC'] = paired['pseq_f'].apply(gc_fraction)\n",
    "    paired.loc[:, 'f_Tm'] = paired['pseq_f'].apply(get_tm)\n",
    "    paired.loc[:, 'f_indel'] = paired.apply(lambda row:\n",
    "        row['pseq_f'].count('-') + row['tseq_f'].count('-'), axis=1)\n",
    "    paired.loc[:, 'f_mm'] = paired.apply(lambda row:\n",
    "        sum((c1 != c2) and (c1 != '-') and (c2 != '-') for c1, c2 in zip(row['pseq_f'], row['tseq_f'])), axis=1)\n",
    "\n",
    "    paired.loc[:, 'r_len'] = paired['pseq_r'].apply(len)\n",
    "    paired.loc[:, 'r_GC'] = paired['pseq_r'].apply(gc_fraction)\n",
    "    paired.loc[:, 'r_Tm'] = paired['pseq_r'].apply(get_tm)\n",
    "    paired.loc[:, 'r_indel'] = paired.apply(lambda row:\n",
    "        row['pseq_r'].count('-') + row['tseq_r'].count('-'), axis=1)\n",
    "    paired.loc[:, 'r_mm'] = paired.apply(lambda row:\n",
    "        sum((c1 != c2) and (c1 != '-') and (c2 != '-') for c1, c2 in zip(row['pseq_r'], row['tseq_r'])), axis=1)\n",
    "\n",
    "    return paired\n",
    "\n",
    "# start = time.time()\n",
    "# samtbl = pd.read_table(mapped, sep='\\t', names=names)\n",
    "# samtbl['orientation'] = samtbl['orientation'].apply(lambda x: x % 256)\n",
    "# samtbl['ptarget'] = samtbl['pname'].apply(lambda x: x.split('-')[0])\n",
    "# samtbl['forrev'] = samtbl['pname'].apply(lambda x: x.split('-')[1])\n",
    "# groups = [ (tname, grp, viralseqs[tname]) for tname, grp in samtbl.groupby('tname') ]\n",
    "# with Pool(processes=cpu_count()) as pool:\n",
    "#     results = list(tqdm(pool.map(pair_primers, groups), total=len(groups)))\n",
    "#     filtered_results = [df for df in results if df is not None]\n",
    "#     inptbl = pd.concat(filtered_results, ignore_index=True)\n",
    "# inptbl = inptbl.drop_duplicates(['pseq_f','tseq_f','pseq_r','tseq_r','prod_len'])\n",
    "# runtime = time.time() - start\n",
    "# print('# pairs: %i' % len(inptbl))\n",
    "# print('Run time (parsing): %.3f sec' % runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_row(row):\n",
    "    fenc = one_hot_encode(row['pseq_f'])\n",
    "    ftenc = one_hot_encode(row['tseq_f'])\n",
    "    renc = one_hot_encode(row['pseq_r'])\n",
    "    rtenc = one_hot_encode(row['tseq_r'])\n",
    "    prienc = np.append(fenc, renc, axis=0)   # Primer\n",
    "    tarenc = np.append(ftenc, rtenc, axis=0) # Target\n",
    "    combined = np.append(tarenc, prienc, axis=1)\n",
    "    return combined\n",
    "\n",
    "def one_hot_encode_pbs_gap_parallel(df_seqs):\n",
    "    rows = df_seqs.to_dict('records')\n",
    "    with Pool(processes=cpu_count()) as pool:\n",
    "        results = pool.map(encode_row, rows)\n",
    "\n",
    "    final_encoded = np.array(results)  # (batch, 56, 10)\n",
    "    print(final_encoded.shape)\n",
    "    return torch.tensor(final_encoded, dtype=torch.float32)\n",
    "\n",
    "# start = time.time()\n",
    "# seq_input = one_hot_encode_pbs_gap_parallel(inptbl)\n",
    "# fea_input = scaler.transform(inptbl[FEATS])\n",
    "# labels = np.array([0]*len(inptbl))\n",
    "# dataset = PcrDataset(seq_input, fea_input, labels)\n",
    "# loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "# runtime = (time.time() - start)\n",
    "# print('Run time (encoding): %.3f sec' % runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "inpf = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/origene_primers_0731.viral.csv'\n",
    "\n",
    "start = time.time()\n",
    "predict_cls, predict_reg = [], []\n",
    "with torch.no_grad():\n",
    "    for seq_in, fea_in, lab in tqdm(loader, desc='Infering'):\n",
    "        seq_in, fea_in, lab = seq_in.to(device).float(), fea_in.to(device).float(), lab.to(device).float()\n",
    "        out_cls = classifier(seq_in)\n",
    "        out_reg = model(fea_in, seq_in)\n",
    "        if len(seq_in)==1:\n",
    "            predict_cls.append(np.array([out_cls.squeeze().detach().cpu().numpy()]))\n",
    "            predict_reg.append(np.array([out_reg.squeeze().detach().cpu().numpy()]))\n",
    "        else:\n",
    "            predict_cls.append(out_cls.squeeze().detach().cpu().numpy())\n",
    "            predict_reg.append(out_reg.squeeze().detach().cpu().numpy())\n",
    "    predict_cls = np.concatenate(predict_cls)\n",
    "    predict_reg = np.concatenate(predict_reg)\n",
    "inptbl.loc[:,'classifier'] = predict_cls\n",
    "inptbl.loc[:,'regressor'] = predict_reg\n",
    "inptbl.to_csv(inpf, index=False)\n",
    "runtime = (time.time() - start)\n",
    "print('Run time (inferece): %.3f sec' % runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(5,2), sharey=True)\n",
    "ax1.hist(inptbl['classifier'])\n",
    "ax2.hist(inptbl['regressor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping to human sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usednmf = '/home/jupyter/ADAPT_PCR_share/safe/resources/origene_primers_nms.fa' \n",
    "# with open(usednmf, 'wt') as out:\n",
    "#     for nm in oritbl['NM']:\n",
    "#         out.write('>%s\\n%s\\n' % (nm, nmseqs[nm]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapopt = '--mp 4,4 --rdg 4,4 --rfg 4,4 -L 8 -N 1 --score-min L,-0.6,-0.6'\n",
    "threads = 4\n",
    "ref = '/home/jupyter/ADAPT_PCR_share/safe/resources/genomes/bowtie2/origene_primers_nms'\n",
    "sam = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/origene_primers_0806.human.sam'\n",
    "multimap = 5000\n",
    "command = '%s -x %s -U %s -f -p %i -k %i %s --no-hd --no-unal > %s'\\\n",
    "           % (bowtie2, ref, fasta, threads, multimap, mapopt, sam)\n",
    "#!$command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = sam.replace('.sam','.parsed')\n",
    "pseqs = parsed + '.pseq'\n",
    "tseqs = parsed + '.tseq'\n",
    "mapped = sam.replace('.sam','.mapped')\n",
    "\n",
    "comm1 = '%s < %s > %s' % (sam2pairwise, sam, parsed)\n",
    "comm2 = 'awk \"NR %% 4 == 2\" %s > %s' % (parsed, pseqs)\n",
    "comm3 = 'awk \"NR %% 4 == 0\" %s > %s' % (parsed, tseqs)\n",
    "comm4 = 'cut -f1-4 %s | paste - %s | paste - %s > %s' % (sam, pseqs, tseqs, mapped)\n",
    "\n",
    "# !$comm1\n",
    "# !$comm2\n",
    "# !$comm3\n",
    "# !$comm4\n",
    "# delete([parsed,pseqs,tseqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minl, maxl = 50, 10000\n",
    "names = ['pname','orientation','tname','start','pseq','tseq']\n",
    "subcols = ['pname','start','pseq','tseq','ptarget']\n",
    "\n",
    "start = time.time()\n",
    "samtbl = pd.read_table(mapped, sep='\\t', names=names)\n",
    "samtbl['orientation'] = samtbl['orientation'].apply(lambda x: x % 256)\n",
    "samtbl['ptarget'] = samtbl['pname'].apply(lambda x: x.split('-')[0])\n",
    "samtbl['forrev'] = samtbl['pname'].apply(lambda x: x.split('-')[1])\n",
    "groups = [ (tname, grp, nmseqs[tname]) for tname, grp in samtbl.groupby('tname') ]\n",
    "with Pool(processes=cpu_count()) as pool:\n",
    "    results = list(tqdm(pool.map(pair_primers, groups), total=len(groups)))\n",
    "    filtered_results = [df for df in results if df is not None]\n",
    "    inptbl = pd.concat(filtered_results, ignore_index=True)\n",
    "tmptbl = inptbl.drop_duplicates(['pseq_f','tseq_f','pseq_r','tseq_r','prod_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = 100\n",
    "allts = set(tmptbl['ptarget'])\n",
    "plens = tmptbl[(tmptbl['ptarget']==tmptbl['tname'])]\\\n",
    ".sort_values('prod_len').drop_duplicates('ptarget')\\\n",
    ".set_index('ptarget')['prod_len'].to_dict()\n",
    "for target in allts-set(plens.keys()):\n",
    "    plens[target] = 150\n",
    "        \n",
    "inptbl = tmptbl[(tmptbl['ptarget']!=tmptbl['tname'])]\n",
    "inptbl = inptbl[inptbl.apply(lambda row: row['prod_len']>plens[row['ptarget']]+diff, axis=1)]\n",
    "runtime = time.time() - start\n",
    "print('# pairs: %i' % len(inptbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "seq_input = one_hot_encode_pbs_gap_parallel(inptbl)\n",
    "fea_input = scaler.transform(inptbl[FEATS])\n",
    "labels = np.array([0]*len(inptbl))\n",
    "dataset = PcrDataset(seq_input, fea_input, labels)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "runtime = (time.time() - start)\n",
    "print('Run time (encoding): %.3f sec' % runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "inpf = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/origene_primers_0806.human.csv'\n",
    "\n",
    "start = time.time()\n",
    "predict_cls, predict_reg = [], []\n",
    "with torch.no_grad():\n",
    "    for seq_in, fea_in, lab in tqdm(loader, desc='Infering'):\n",
    "        seq_in, fea_in, lab = seq_in.to(device).float(), fea_in.to(device).float(), lab.to(device).float()\n",
    "        out_cls = classifier(seq_in)\n",
    "        out_reg = model(fea_in, seq_in)\n",
    "        if len(seq_in)==1:\n",
    "            predict_cls.append(np.array([out_cls.squeeze().detach().cpu().numpy()]))\n",
    "            predict_reg.append(np.array([out_reg.squeeze().detach().cpu().numpy()]))\n",
    "        else:\n",
    "            predict_cls.append(out_cls.squeeze().detach().cpu().numpy())\n",
    "            predict_reg.append(out_reg.squeeze().detach().cpu().numpy())\n",
    "    predict_cls = np.concatenate(predict_cls)\n",
    "    predict_reg = np.concatenate(predict_reg)\n",
    "inptbl.loc[:,'classifier'] = predict_cls\n",
    "inptbl.loc[:,'regressor'] = predict_reg\n",
    "inptbl.to_csv(inpf, index=False)\n",
    "runtime = (time.time() - start)\n",
    "print('Run time (inferece): %.3f sec' % runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truef = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/origene_primers_0806.true.csv'\n",
    "# falsef1 = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/origene_primers_0806.false.csv'\n",
    "# falsef2 = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/origene_primers_0731.viral.csv'\n",
    "# falsef3 = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/origene_primers_0806.human.csv'\n",
    "truef = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/origene_primers.true.dg.csv'\n",
    "falsef = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/origene_primers.false.dg.csv'\n",
    "truetbl = pd.read_csv(truef, index_col=0)\n",
    "falsetbl = pd.read_csv(falsef, index_col=0)\n",
    "print('# true set: %i, # false set: %i' % (len(truetbl), len(falsetbl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = pd.read_csv('/home/jupyter/ADAPT_PCR_share/safe/dataset/0717_dataset_test.csv',index_col=[0,1])\n",
    "truetbl = get_pair_dg_parallel(test_df[test_df['binary']==1])\n",
    "falsetbl = get_pair_dg_parallel(test_df[test_df['binary']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truef = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/test_set.true.dg.csv'\n",
    "falsef = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/test_set.false.dg.csv'\n",
    "truetbl.to_csv(truef,index=False)\n",
    "falsetbl.to_csv(falsef,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmcols = ['f_mm','f_indel','r_mm','r_indel']\n",
    "dgcols = ['f_dg','r_dg']\n",
    "for tbl in [truetbl, falsetbl]:\n",
    "    tbl['mismatches'] = tbl[mmcols].sum(axis=1)\n",
    "    tbl['dg'] = tbl[dgcols].max(axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = defaultdict(dict)\n",
    "pcols = ['mismatches','dg','classifier','regressor']\n",
    "plabels = ['Distance-based', 'Free energy-based', 'Our classifier', 'Our regressor']\n",
    "efuncs = [roc_auc_score, average_precision_score]\n",
    "elabels = ['auROC', 'auPRC']\n",
    "for col, plabel in zip(pcols, plabels):\n",
    "    for func, elabel in zip(efuncs, elabels):\n",
    "        pos_scores = truetbl[col].tolist()\n",
    "        neg_scores = falsetbl[col].tolist()\n",
    "        y_true = np.array([1]*len(pos_scores) + [0]*len(neg_scores)) \n",
    "        y_score = np.concatenate([pos_scores, neg_scores])\n",
    "        score = func(y_true, y_score)\n",
    "        scores[plabel][elabel] = max(1-score, score)\n",
    "scores = pd.DataFrame(scores).T\n",
    "scores.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores.to_csv('/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/origene_primers.eval.csv',index=False)\n",
    "scores.to_csv('/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/test_set.eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['target_seq', 'f_name', 'f_seq', 'f_start', 'f_end', 'f_len', 'f_Tm',\n",
    "       'f_GC', 'f_indel', 'f_mm', 'pseq_f', 'tseq_f', 'r_name', 'r_seq',\n",
    "       'r_start', 'r_end', 'r_len', 'r_Tm', 'r_GC', 'r_indel', 'r_mm',\n",
    "       'pseq_r', 'tseq_r', 'prod_len', 'prod_Tm', 'score', 'binary']\n",
    "test_df.columns = cols\n",
    "start = time.time()\n",
    "seq_input = one_hot_encode_pbs_gap_parallel(test_df)\n",
    "fea_input = scaler.transform(test_df[FEATS])\n",
    "labels = np.array([0]*len(test_df))\n",
    "dataset = PcrDataset(seq_input, fea_input, labels)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "runtime = (time.time() - start)\n",
    "print('Run time (encoding): %.3f sec' % runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "predict_cls, predict_reg = [], []\n",
    "with torch.no_grad():\n",
    "    for seq_in, fea_in, lab in tqdm(loader, desc='Infering'):\n",
    "        seq_in, fea_in, lab = seq_in.to(device).float(), fea_in.to(device).float(), lab.to(device).float()\n",
    "        out_cls = classifier(seq_in)\n",
    "        out_reg = model(fea_in, seq_in)\n",
    "        if len(seq_in)==1:\n",
    "            predict_cls.append(np.array([out_cls.squeeze().detach().cpu().numpy()]))\n",
    "            predict_reg.append(np.array([out_reg.squeeze().detach().cpu().numpy()]))\n",
    "        else:\n",
    "            predict_cls.append(out_cls.squeeze().detach().cpu().numpy())\n",
    "            predict_reg.append(out_reg.squeeze().detach().cpu().numpy())\n",
    "    predict_cls = np.concatenate(predict_cls)\n",
    "    predict_reg = np.concatenate(predict_reg)\n",
    "test_df.loc[:,'classifier'] = predict_cls\n",
    "test_df.loc[:,'regressor'] = predict_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_dg(row):\n",
    "    row['f_dg'] = get_dg_vienna(row['pseq_f'], reverse_complement_dna(row['tseq_f']))\n",
    "    row['r_dg'] = get_dg_vienna(row['pseq_r'], reverse_complement_dna(row['tseq_r']))\n",
    "    return row\n",
    "\n",
    "def get_pair_dg_parallel(df):\n",
    "    rows = df.to_dict('records')\n",
    "    with Pool(processes=cpu_count()) as pool:\n",
    "        results = pool.map(get_pair_dg, rows)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# true_dg = get_pair_dg_parallel(truetbl)\n",
    "# false_dg = get_pair_dg_parallel(falsetbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truef = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/test_set.true.dg.csv'\n",
    "falsef = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/test_set.false.dg.csv'\n",
    "#truetbl.to_csv(truef,index=False)\n",
    "#falsetbl.to_csv(falsef,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truef = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/origene_primers.true.dg.csv'\n",
    "# falsef = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/origene_primers.false.dg.csv'\n",
    "# truetbl.to_csv(truef,index=False)\n",
    "# falsetbl.to_csv(falsef,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_primer3_check(templ, fseq, rseq, pname, path):\n",
    "    inptxt = '''SEQUENCE_ID=example_check\n",
    "SEQUENCE_TEMPLATE=%s\n",
    "PRIMER_TASK=check_primers\n",
    "PRIMER_EXPLAIN_FLAG=1\n",
    "SEQUENCE_PRIMER=%s\n",
    "SEQUENCE_PRIMER_REVCOMP=%s\n",
    "PRIMER_MIN_SIZE=15\n",
    "PRIMER_OPT_SIZE=20\n",
    "PRIMER_MAX_SIZE=27\n",
    "PRIMER_MIN_GC=20.0\n",
    "PRIMER_OPT_GC_PERCENT=50.0\n",
    "PRIMER_MAX_GC=80.0\n",
    "PRIMER_MIN_TM=50.0\n",
    "PRIMER_OPT_TM=60.0\n",
    "PRIMER_MAX_TM=72.0\n",
    "PRIMER_PRODUCT_SIZE_RANGE=50-10000\n",
    "PRIMER_MAX_SELF_ANY=12\n",
    "PRIMER_MAX_SELF_END=8\n",
    "PRIMER_PAIR_MAX_COMPL_ANY=12\n",
    "PRIMER_PAIR_MAX_COMPL_END=8\n",
    "PRIMER_MAX_POLY_X=5\n",
    "PRIMER_MAX_HAIRPIN_TH=47.0\n",
    "=''' % (templ, fseq, rseq)\n",
    "    inpfile = '%s/%s.txt' % (path, pname)\n",
    "    with open(inpfile, 'wt') as out:\n",
    "        out.write(inptxt)\n",
    "    result = !$primer3 $inpfile\n",
    "    \n",
    "    left_penalty = None\n",
    "    right_penalty = None\n",
    "    pair_penalty = None\n",
    "    for line in result:\n",
    "        if line.startswith(\"PRIMER_LEFT_0_PENALTY\"):\n",
    "            left_penalty = float(line.split(\"=\")[1])\n",
    "        elif line.startswith(\"PRIMER_RIGHT_0_PENALTY\"):\n",
    "            right_penalty = float(line.split(\"=\")[1])\n",
    "        elif line.startswith(\"PRIMER_PAIR_0_PENALTY\"):\n",
    "            pair_penalty = float(line.split(\"=\")[1])\n",
    "\n",
    "    return left_penalty, right_penalty, pair_penalty\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pname = 0\n",
    "off = 5\n",
    "fseq = truetbl.loc[pname,'pseq_f']\n",
    "rseq = truetbl.loc[pname,'pseq_r']\n",
    "\n",
    "tseq = truetbl.loc[pname,'tseq']\n",
    "fst = tseq.find(fseq)\n",
    "rst = fst + tseq[fst:].find(reverse_complement_dna(rseq))\n",
    "templ = tseq[fst-off:rst + len(rseq)+off]\n",
    "\n",
    "path = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/temp'\n",
    "\n",
    "\n",
    "\n",
    "#run_primer3_check(templ, fseq, rseq, pname, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inptxt = '''SEQUENCE_ID=example_check\n",
    "SEQUENCE_TEMPLATE=%s\n",
    "PRIMER_TASK=check_primers\n",
    "PRIMER_EXPLAIN_FLAG=1\n",
    "SEQUENCE_PRIMER=%s\n",
    "SEQUENCE_PRIMER_REVCOMP=%s\n",
    "PRIMER_MIN_SIZE=15\n",
    "PRIMER_OPT_SIZE=20\n",
    "PRIMER_MAX_SIZE=27\n",
    "PRIMER_MIN_GC=20.0\n",
    "PRIMER_OPT_GC_PERCENT=50.0\n",
    "PRIMER_MAX_GC=80.0\n",
    "PRIMER_MIN_TM=50.0\n",
    "PRIMER_OPT_TM=60.0\n",
    "PRIMER_MAX_TM=72.0\n",
    "PRIMER_PRODUCT_SIZE_RANGE=50-1000\n",
    "PRIMER_MAX_SELF_ANY=12\n",
    "PRIMER_MAX_SELF_END=8\n",
    "PRIMER_PAIR_MAX_COMPL_ANY=12\n",
    "PRIMER_PAIR_MAX_COMPL_END=8\n",
    "PRIMER_MAX_POLY_X=5\n",
    "PRIMER_MAX_HAIRPIN_TH=47.0\n",
    "=''' % (templ, fseq, rseq)\n",
    "inpfile = '%s/%s.txt' % (path, pname)\n",
    "with open(inpfile, 'wt') as out:\n",
    "    out.write(inptxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$primer3 $inpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inptxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truetbl.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/home/jupyter/ADAPT_PCR_share/safe/dataset/0717_dataset_train.csv',index_col=[0,1])\n",
    "valid_df = pd.read_csv('/home/jupyter/ADAPT_PCR_share/safe/dataset/0717_dataset_valid.csv',index_col=[0,1])\n",
    "test_df = pd.read_csv('/home/jupyter/ADAPT_PCR_share/safe/dataset/0717_dataset_test.csv',index_col=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_dg(row):\n",
    "    row['f_dg'] = get_dg_vienna(row['f_seq'], reverse_complement_dna(row['f_tenc'].replace('-','').upper()))\n",
    "    row['r_dg'] = get_dg_vienna(row['r_seq'], reverse_complement_dna(row['r_tenc'].replace('-','').upper()))\n",
    "    return row\n",
    "test_dgs = get_pair_dg_parallel(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgcols = ['f_dg','r_dg']\n",
    "test_dgs['dg'] = test_dgs[dgcols].max(axis=1)\n",
    "\n",
    "mmcols = ['f_mm','f_indel','r_mm','r_indel']\n",
    "test_dgs['mm'] = test_dgs[mmcols].sum(axis=1)\n",
    "test_dgs.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "truet = test_df[test_df['binary']==1]\n",
    "truedg = test_dgs[test_dgs['binary']==1]\n",
    "print(spearmanr(truedg['mm'], truedg['score']), mean_squared_error(truedg['mm'], truedg['score']))\n",
    "print(spearmanr(truedg['dg'], truedg['score']), mean_squared_error(truedg['dg'], truedg['score']))\n",
    "print(spearmanr(truet['regressor'], truet['score']), mean_squared_error(truet['regressor'], truet['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "truet = test_df[test_df['binary']==1]\n",
    "truedg = test_dgs[test_dgs['binary']==1]\n",
    "print(spearmanr(truedg['mm'], truedg['score']), \n",
    "      mean_squared_error(truedg['mm'], truedg['score'])/np.var(truedg['mm']))\n",
    "print(spearmanr(truedg['dg'], truedg['score']),\n",
    "      mean_squared_error(truedg['dg'], truedg['score'])/np.var(truedg['dg']))\n",
    "print(spearmanr(truet['regressor'], truet['score']), \n",
    "      mean_squared_error(truet['regressor'],truet['score'])/np.var(truet['regressor']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truet = test_df[test_df['binary']==1]\n",
    "falset = test_df[test_df['binary']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmcols = ['f_mm','f_indel','r_mm','r_indel']\n",
    "pos_scores = truet[mmcols].sum(axis=1).tolist()\n",
    "neg_scores = falset[mmcols].sum(axis=1).tolist()\n",
    "\n",
    "y_true = np.array([0]*len(pos_scores) + [1]*len(neg_scores)) \n",
    "y_scores = np.concatenate([pos_scores, neg_scores])\n",
    "auroc = roc_auc_score(y_true, y_scores)\n",
    "print(\"AUROC:\", auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_dg(row):\n",
    "    row['f_dg'] = get_dg_vienna(row['f_seq'], reverse_complement_dna(row['f_tenc'].replace('-','').upper()))\n",
    "    row['r_dg'] = get_dg_vienna(row['r_seq'], reverse_complement_dna(row['r_tenc'].replace('-','').upper()))\n",
    "    return row\n",
    "\n",
    "truet_dg = get_pair_dg_parallel(truet)\n",
    "falset_dg = get_pair_dg_parallel(falset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgcols = ['f_dg','r_dg']\n",
    "pos_scores = truet_dg[dgcols].max(axis=1).tolist()\n",
    "neg_scores = falset_dg[dgcols].max(axis=1).tolist()\n",
    "\n",
    "y_true = np.array([1]*len(pos_scores) + [0]*len(neg_scores)) \n",
    "y_scores = np.concatenate([pos_scores, neg_scores])\n",
    "auroc = roc_auc_score(y_true, y_scores)\n",
    "print(\"AUROC:\", max(auroc, 1-auroc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kayama et al. 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prif = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/kayama_2021/primers_kayama_2021.csv'\n",
    "# primers = pd.read_csv(prif, header=1, names=['name','seq'])\n",
    "# primers = {i:row['seq'].strip().replace(' ','') for i,row in primers.iterrows()}\n",
    "prif = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/kayama_2021/primers.fa'\n",
    "primerseqs = {s.id:str(s.seq) for s in SeqIO.parse(prif,'fasta')}\n",
    "# with open(prif,'wt') as out:\n",
    "#     for i in range(72):\n",
    "#         out.write('>%i_f\\n%s\\n>%i_r\\n%s\\n'%(i+1,primers[2*i],i+1,primers[2*i+1]))\n",
    "targetf =  '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/kayama_2021/targets.fa'\n",
    "targetseqs = {s.id:str(s.seq) for s in SeqIO.parse(targetf,'fasta')}\n",
    "print(len(primerseqs), len(targetseqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_match(seq, ref, dcut=10):\n",
    "    if len(ref) < len(seq):\n",
    "        print('x')\n",
    "        return (-1, 999) \n",
    "    if seq in ref:\n",
    "        return (ref.find(seq), 0)\n",
    "    sortvs = sorted([(distance(seq, ref[i:i+len(seq)]), i) for i in range(len(ref)-len(seq))])\n",
    "    d, st = sortvs[0]\n",
    "    return (st, d) if d <= dcut else (-1, 999)\n",
    "\n",
    "dists = defaultdict(lambda: defaultdict(int))\n",
    "for pname, pseq in primerseqs.items():\n",
    "    pid, ori = pname.split('_')\n",
    "    for tname, tseq in targetseqs.items():\n",
    "        if ori=='f':\n",
    "            st, d = find_match(pseq, tseq)\n",
    "        else:\n",
    "            st, d = find_match(reverse_complement_dna(pseq), tseq)\n",
    "        dists[tname][pid] += d\n",
    "        \n",
    "dists = pd.DataFrame(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelf = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/kayama_2021/labels_kayama_2021.csv'\n",
    "labtbl = pd.read_csv(labelf, index_col=0)\n",
    "labtbl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array(labtbl).flatten()\n",
    "ys = np.array(dists).flatten()\n",
    "\n",
    "ys0 = [y for x,y in zip(xs,ys) if x==0]\n",
    "ys1 = [y for x,y in zip(xs,ys) if x==1]\n",
    "\n",
    "bp = plt.boxplot([ys0,ys1], sym='+', widths=.8)\n",
    "plt.xticks([1,2],['No PCR','PCR'])\n",
    "plt.ylabel('Primer-target distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
