{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, r2_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import glob\n",
    "\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = '/home/jupyter/ADAPT_PCR_share/safe/dataset'\n",
    "!ls $DATAPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('%s/0716_dataset_train.csv'%DATAPATH,index_col=[0,1])\n",
    "val_df = pd.read_csv('%s/0716_dataset_valid.csv'%DATAPATH,index_col=[0,1])\n",
    "test_df = pd.read_csv('%s/0716_dataset_test.csv'%DATAPATH,index_col=[0,1])\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "print(test_df.shape)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None) # Adjust display width for long lines\n",
    "pd.set_option('display.max_colwidth', None) # Display full content of cells\n",
    "\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = train_df.columns\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_df['prod_Tm'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handcrafted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = train_df[[\"f_length\", \"f_indel\",\"f_mm\",\"r_length\", \"r_indel\",\"r_mm\",\"f_Tm\", \"r_Tm\", \"prod_length\", \"prod_Tm\"]]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(inputs)\n",
    "X_train_scaled = scaler.transform(inputs)\n",
    "X_val_scaled = scaler.transform(val_df[[\"f_length\", \"f_indel\",\"f_mm\",\"r_length\", \"r_indel\",\"r_mm\",\"f_Tm\", \"r_Tm\", \"prod_length\", \"prod_Tm\"]])\n",
    "X_test_scaled = scaler.transform(test_df[[\"f_length\", \"f_indel\",\"f_mm\",\"r_length\", \"r_indel\",\"r_mm\",\"f_Tm\", \"r_Tm\", \"prod_length\", \"prod_Tm\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# results_df = pd.DataFrame()\n",
    "\n",
    "cutoff = 0 # thresholds we will use to classify as active or inactive\n",
    "results_df = pd.DataFrame()\n",
    "y_train = np.where(train_df['score'] > cutoff, 1, 0) # if it is greater or equal to the threshold, we call it 0, inactive\n",
    "y_test = np.where(test_df['score'] > cutoff, 1, 0)\n",
    "y_val = np.where(val_df['score'] > cutoff, 1, 0)\n",
    "\n",
    "\n",
    "def get_performance(results_df, pipeline, grid, name):\n",
    "    results = []\n",
    "    # Use AUROC as the scoring metric\n",
    "    scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "    # Grid search with 5-fold cross-validation\n",
    "    grid = GridSearchCV(pipeline, param_grid, cv=5, scoring=scorer)\n",
    "    grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Output best model and AUROC\n",
    "#     print(f\"Best C: {grid.best_params_['logreg__C']}\")\n",
    "    print(f\"Best AUROC: {grid.best_score_:.4f}\")\n",
    "    clf = grid.best_estimator_\n",
    "\n",
    "#     mean_scores = grid.cv_results_['mean_test_score']\n",
    "#     std_scores = grid.cv_results_['std_test_score']\n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    if name != \"SVM\":\n",
    "        y_proba = clf.predict_proba(X_test_scaled)\n",
    "        # ROC AUC\n",
    "        roc = roc_auc_score(y_test, y_proba[:, 1])\n",
    "        # ROC Curve\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_proba[:, 1])\n",
    "        plt.plot(fpr, tpr)\n",
    "    else:\n",
    "        y_score = clf.decision_function(X_test_scaled)\n",
    "        roc = roc_auc_score(y_test, y_score)\n",
    "\n",
    "    # Store metrics\n",
    "    results.append({\n",
    "        'Cutoff': cutoff,\n",
    "        'Model': name,\n",
    "        'Regularization': grid.best_params_,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "        'Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "        'F1 Score': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'ROC AUC': roc\n",
    "    })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "#l2    \n",
    "pipeline = Pipeline([('logreg', LogisticRegression(max_iter=1000, penalty='l2', solver='liblinear'))])\n",
    "param_grid = {'logreg__C': np.logspace(-5, 5, num=10)}\n",
    "results_df = pd.concat([results_df, get_performance(results_df, pipeline, param_grid, \"L2\")])\n",
    "#l1\n",
    "pipeline = Pipeline([('logreg', LogisticRegression(max_iter=1000, penalty='l1', solver='liblinear'))])\n",
    "param_grid = {'logreg__C': np.logspace(-5, 5, num=10)}\n",
    "results_df = pd.concat([results_df, get_performance(results_df, pipeline, param_grid, \"L1\")])\n",
    "#l1 + l2\n",
    "pipeline = Pipeline([('logreg', LogisticRegression(max_iter=1000, penalty='elasticnet', solver='saga'))])\n",
    "param_grid = {\n",
    "    'logreg__C': np.logspace(-5, 5, num=10),\n",
    "    'logreg__l1_ratio': np.linspace(0, 1, num=5)\n",
    "}\n",
    "results_df = pd.concat([results_df, get_performance(results_df, pipeline, param_grid, \"Elastic Net\")])\n",
    "# random forest\n",
    "pipeline = Pipeline([('RF', RandomForestClassifier())])\n",
    "param_grid = {\n",
    "    'RF__n_estimators': np.logspace(1, 3, num=5, dtype=int)\n",
    "}\n",
    "results_df = pd.concat([results_df, get_performance(results_df, pipeline, param_grid, \"RF\")])\n",
    "# GB\n",
    "pipeline = Pipeline([('GB', GradientBoostingClassifier())])\n",
    "param_grid = {\n",
    "    'GB__n_estimators': np.logspace(1, 3, num=5, dtype=int),\n",
    "    'GB__learning_rate': np.logspace(-4, 1, num=5)\n",
    "}\n",
    "results_df = pd.concat([results_df, get_performance(results_df, pipeline, param_grid, \"GB\")])\n",
    "# mlp\n",
    "pipeline = Pipeline([('MLP', MLPClassifier(max_iter=1000))])\n",
    "param_grid = {\n",
    "    'MLP__hidden_layer_sizes': [(50),(100),(50, 100)],\n",
    "    'MLP__alpha': np.logspace(-5, 0, num=10)\n",
    "}\n",
    "results_df = pd.concat([results_df, get_performance(results_df, pipeline, param_grid, \"MLP\")])\n",
    "# svm\n",
    "pipeline = Pipeline([('SVM', LinearSVC())])\n",
    "param_grid = {\n",
    "    'SVM__C': np.logspace(-3, 3, num=10)\n",
    "}\n",
    "results_df = pd.concat([results_df, get_performance(results_df, pipeline, param_grid, \"SVM\")])\n",
    "# knn\n",
    "pipeline = Pipeline([('KNN', KNeighborsClassifier())])\n",
    "param_grid = {\n",
    "    'KNN__n_neighbors': np.linspace(0, 20, num=5),\n",
    "    'KNN__weights': ['uniform', 'distance']\n",
    "}\n",
    "results_df = pd.concat([results_df, get_performance(results_df, pipeline, param_grid, \"KNN\")])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-based classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCRDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.Tensor(np.array(X)) \n",
    "        self.y = torch.Tensor(np.array(y))\n",
    "        self.len=len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXPLEN = max(train_df['f_penc'].apply(len).max(), train_df['r_penc'].apply(len).max())\n",
    "print(MAXPLEN)\n",
    "\n",
    "def one_hot_encode(seq, length=28):\n",
    "    mapping = { 'A':[1, 0, 0, 0, 0],\n",
    "                'T':[0, 1, 0, 0, 0],\n",
    "                'C':[0, 0, 1, 0, 0],\n",
    "                'G':[0, 0, 0, 1, 0],\n",
    "                'N':[0, 0, 0, 0, 0],\n",
    "                '-':[0, 0, 0, 0, 1] }\n",
    "    seq = seq.ljust(length, 'N') # (6, ATCG) -> NNATCG\n",
    "    return np.array([mapping[char.upper()] for char in seq])\n",
    "\n",
    "def one_hot_encode_full_gap(df_seqs, maxl=1421):\n",
    "    primer_encoded = []\n",
    "    target_encoded = []\n",
    "    for (tname,pname),row in df_seqs.iterrows():\n",
    "        fseq, fst, rseq, rst, tseq = row[['f_seq','f_start','r_seq','r_start','target_seq']]\n",
    "        fenc, ftenc, renc, rtenc = row[['f_penc','f_tenc','r_penc','r_tenc']]\n",
    "        pseq = 'N'*fst + fenc + 'N'*(rst-(fst+len(fseq))) + renc + 'N'*(len(tseq)-(rst+len(rseq)))\n",
    "        tseq = tseq[:fst] + ftenc + tseq[fst+len(fseq):rst] + rtenc + tseq[rst+len(rseq):]\n",
    "        primer_encoded.append(one_hot_encode(pseq, maxl))\n",
    "        target_encoded.append(one_hot_encode(tseq, maxl))\n",
    "    final_encoded = np.append(np.array(target_encoded), np.array(primer_encoded), axis=2)\n",
    "    print(final_encoded.shape)\n",
    "    return torch.tensor(final_encoded, dtype=torch.float32)\n",
    "\n",
    "def one_hot_encode_pbs_gap(df_seqs):\n",
    "    primer_encoded = []\n",
    "    target_encoded = []\n",
    "    for (tname,pname),row in df_seqs.iterrows():\n",
    "        fenc, ftenc, renc, rtenc = row[['f_penc','f_tenc','r_penc','r_tenc']].apply(one_hot_encode)\n",
    "        prienc = np.append(fenc,renc,axis=0)\n",
    "        tarenc = np.append(ftenc,rtenc,axis=0)\n",
    "        primer_encoded.append(prienc)\n",
    "        target_encoded.append(tarenc)\n",
    "    primer_encoded = np.array(primer_encoded)\n",
    "    target_encoded = np.array(target_encoded)\n",
    "    final_encoded = np.append(target_encoded, primer_encoded, axis=2)\n",
    "    print(final_encoded.shape)\n",
    "    return torch.tensor(final_encoded, dtype=torch.float32)\n",
    "\n",
    "# Class that is necessary to access Dataloaders and other Pytorch utilities.\n",
    "class PCRDataset(Dataset):\n",
    "    def __init__(self, encoded_input, ct_values):\n",
    "        \"\"\"\n",
    "        encoded_input: consists of a tensor containing (8 x (len of target sequence) encoding of the sequence)\n",
    "        the upper 4 rows are the one-hot encoding of the primer sequences, the lower 4 rows are the one-hot encoding of the target sequence\n",
    "        custom_features: a tensor containing the computed thermodynamic features for each primer pair and target sequence\n",
    "        ct_values:\n",
    "        a tensor containing the ct values for each primer pair and target sequence\n",
    "\n",
    "        \"\"\"\n",
    "        self.encoded_input = encoded_input\n",
    "        self.ct_values = ct_values\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_input)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_input[idx], self.ct_values[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change between these for either full-sequence or core sequence testing\n",
    "\n",
    "\n",
    "# train_inps = one_hot_encode_sequences(train_df)\n",
    "# val_inps = one_hot_encode_sequences(val_df)\n",
    "# test_inps = one_hot_encode_sequences(test_df)\n",
    "\n",
    "# train_inps = one_hot_encode_full_gap(train_df)\n",
    "# val_inps = one_hot_encode_full_gap(val_df)\n",
    "# test_inps = one_hot_encode_full_gap(test_df)\n",
    "# # useful for cross-validation\n",
    "# train_and_val_inps = torch.cat((train_inps,val_inps))\n",
    "\n",
    "cutoff=0\n",
    "\n",
    "train_labels = torch.tensor(np.where(train_df['score'] > cutoff, 1, 0))\n",
    "val_labels = torch.tensor(np.where(val_df['score'] > cutoff, 1, 0))\n",
    "test_labels = torch.tensor(np.where(test_df['score'] > cutoff, 1, 0))\n",
    "train_val_labels = torch.cat((train_labels, val_labels))\n",
    "\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = PCRDataset(train_inps, train_labels)\n",
    "val_dataset = PCRDataset(val_inps, val_labels)\n",
    "test_dataset = PCRDataset(test_inps, test_labels)\n",
    "train_val_dataset = PCRDataset(train_and_val_inps, train_val_labels)\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) # can play around\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "train_val_dataset = DataLoader(train_val_dataset, batch_size=64, shuffle=False) # dont want it interfering with cross-val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import dump, load\n",
    "from tqdm.auto import tqdm\n",
    "from Bio import Seq\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified output layer, loss to be used for classification\n",
    "\n",
    "class PGC(nn.Module):\n",
    "    def __init__(self,d_model,expansion_factor = 1.0,dropout = 0.0):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.dropout = dropout\n",
    "        self.conv = nn.Conv1d(int(d_model * expansion_factor), int(d_model * expansion_factor),\n",
    "                              kernel_size=3, padding=1, groups=int(d_model * expansion_factor))\n",
    "        self.in_proj = nn.Linear(d_model, int(d_model * expansion_factor * 2))\n",
    "        self.out_norm = nn.RMSNorm(int(d_model), eps=1e-8)\n",
    "        self.in_norm = nn.RMSNorm(int(d_model * expansion_factor * 2), eps=1e-8)\n",
    "        self.out_proj = nn.Linear(int(d_model * expansion_factor), d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, u):\n",
    "        xv = self.in_norm(self.in_proj(u))\n",
    "        x,v = xv.chunk(2,dim=-1)\n",
    "        x_conv = self.conv(x.transpose(-1,-2)).transpose(-1,-2)\n",
    "        gate =  v * x_conv\n",
    "        x = self.out_norm(self.out_proj(gate))\n",
    "        return x\n",
    "    \n",
    "class DropoutNd(nn.Module):\n",
    "    def __init__(self, p: float = 0.5, tie=True, transposed=True):\n",
    "        \"\"\"\n",
    "        tie: tie dropout mask across sequence lengths (Dropout1d/2d/3d)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if p < 0 or p >= 1:\n",
    "            raise ValueError(\"dropout probability has to be in [0, 1), \" \"but got {}\".format(p))\n",
    "        self.p = p\n",
    "        self.tie = tie\n",
    "        self.transposed = transposed\n",
    "        self.binomial = torch.distributions.binomial.Binomial(probs=1-self.p)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"X: (batch, dim, lengths...).\"\"\"\n",
    "        if self.training:\n",
    "            if not self.transposed: X = rearrange(X, 'b ... d -> b d ...')\n",
    "            # binomial = torch.distributions.binomial.Binomial(probs=1-self.p) \n",
    "            # This is incredibly slow because of CPU -> GPU copying\n",
    "            mask_shape = X.shape[:2] + (1,)*(X.ndim-2) if self.tie else X.shape\n",
    "            # mask = self.binomial.sample(mask_shape)\n",
    "            mask = torch.rand(*mask_shape, device=X.device) < 1.-self.p\n",
    "            X = X * mask * (1.0/(1-self.p))\n",
    "            if not self.transposed: X = rearrange(X, 'b d ... -> b ... d')\n",
    "            return X\n",
    "        return X\n",
    "\n",
    "class S4DKernel(nn.Module):\n",
    "    \"\"\"Generate convolution kernel from diagonal SSM parameters.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, N=64, dt_min=0.001, dt_max=0.1, lr=None):\n",
    "        super().__init__()\n",
    "        # Generate dt\n",
    "        H = d_model\n",
    "        log_dt = torch.rand(H) * (\n",
    "            math.log(dt_max) - math.log(dt_min)\n",
    "        ) + math.log(dt_min)\n",
    "\n",
    "        C = torch.randn(H, N // 2, dtype=torch.cfloat)\n",
    "        self.C = nn.Parameter(torch.view_as_real(C))\n",
    "        self.register(\"log_dt\", log_dt, lr)\n",
    "\n",
    "        log_A_real = torch.log(0.5 * torch.ones(H, N//2))\n",
    "        A_imag = math.pi * repeat(torch.arange(N//2), 'n -> h n', h=H)\n",
    "        self.register(\"log_A_real\", log_A_real, lr)\n",
    "        self.register(\"A_imag\", A_imag, lr)\n",
    "\n",
    "    def forward(self, L):\n",
    "        \"\"\"\n",
    "        returns: (..., c, L) where c is number of channels (default 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # Materialize parameters\n",
    "        dt = torch.exp(self.log_dt) # (H)\n",
    "        C = torch.view_as_complex(self.C) # (H N)\n",
    "        A = -torch.exp(self.log_A_real) + 1j * self.A_imag # (H N)\n",
    "\n",
    "        # Vandermonde multiplication\n",
    "        dtA = A * dt.unsqueeze(-1)  # (H N)\n",
    "        K = dtA.unsqueeze(-1) * torch.arange(L, device=A.device) # (H N L)\n",
    "        C = C * (torch.exp(dtA)-1.) / A\n",
    "        K = 2 * torch.einsum('hn, hnl -> hl', C, torch.exp(K)).real\n",
    "\n",
    "        return K\n",
    "\n",
    "    def register(self, name, tensor, lr=None):\n",
    "        \"\"\"Register a tensor with a configurable learning rate and 0 weight decay\"\"\"\n",
    "\n",
    "        if lr == 0.0:\n",
    "            self.register_buffer(name, tensor)\n",
    "        else:\n",
    "            self.register_parameter(name, nn.Parameter(tensor))\n",
    "\n",
    "            optim = {\"weight_decay\": 0.0}\n",
    "            if lr is not None: optim[\"lr\"] = lr\n",
    "            setattr(getattr(self, name), \"_optim\", optim)\n",
    "\n",
    "\n",
    "class S4D(nn.Module):\n",
    "    def __init__(self, d_model, d_state=64, dropout=0.0, transposed=True, **kernel_args):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h = d_model\n",
    "        self.n = d_state\n",
    "        self.d_output = self.h\n",
    "        self.transposed = transposed\n",
    "\n",
    "        self.D = nn.Parameter(torch.randn(self.h))\n",
    "        # SSM Kernel\n",
    "        self.kernel = S4DKernel(self.h, N=self.n, **kernel_args)\n",
    "        # Pointwise\n",
    "        self.activation = nn.GELU()\n",
    "        dropout_fn = DropoutNd\n",
    "        self.dropout = dropout_fn(dropout) if dropout > 0.0 else nn.Identity()\n",
    "\n",
    "        # position-wise output transform to mix features\n",
    "        self.output_linear = nn.Sequential(\n",
    "            nn.Conv1d(self.h, 2*self.h, kernel_size=1),\n",
    "            nn.GLU(dim=-2),\n",
    "        )\n",
    "\n",
    "    def forward(self, u, **kwargs): # absorbs return_output and transformer src mask\n",
    "        \"\"\" Input and output shape (B, H, L) \"\"\"\n",
    "        if not self.transposed: u = u.transpose(-1, -2)\n",
    "        L = u.size(-1)\n",
    "        # Compute SSM Kernel\n",
    "        k = self.kernel(L=L) # (H L)\n",
    "\n",
    "        # Convolution\n",
    "        k_f = torch.fft.rfft(k, n=2*L)  # (H L)\n",
    "        u_f = torch.fft.rfft(u, n=2*L) # (B H L)\n",
    "        y = torch.fft.irfft(u_f*k_f, n=2*L)[..., :L] # (B H L)\n",
    "\n",
    "        # Compute D term in state space equation - essentially a skip connection\n",
    "        y = y + u * self.D.unsqueeze(-1)\n",
    "\n",
    "        y = self.dropout(self.activation(y))\n",
    "        y = self.output_linear(y)\n",
    "        if not self.transposed: y = y.transpose(-1, -2)\n",
    "        return y\n",
    "    \n",
    "class Janus(nn.Module):\n",
    "    def __init__(self, d_input, d_output, d_model, d_state=64, dropout=0.2, transposed=False, **kernel_args):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(d_input, d_model)\n",
    "        self.pgc1 = PGC(d_model, expansion_factor=0.25, dropout=dropout)\n",
    "        self.pgc2 = PGC(d_model, expansion_factor=2, dropout=dropout)\n",
    "        self.s4d = S4D(d_model, d_state=d_state, dropout=dropout, transposed=transposed, **kernel_args)\n",
    "        self.norm = nn.RMSNorm(d_model)\n",
    "        self.decoder = nn.Linear(d_model, d_output)\n",
    "        self.output = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, u):\n",
    "        x = self.encoder(u)\n",
    "        x = self.pgc1(x)\n",
    "        x = self.pgc2(x)\n",
    "        z = x\n",
    "        z = self.norm(z)\n",
    "        x = self.dropout(self.s4d(z)) + x\n",
    "        x = x.mean(dim=1)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.decoder(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def train_loop(model, lr):\n",
    "    metrics = []\n",
    "    num_epochs = 50  # Adjust as needed\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Train Janus model\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01) #lr = 0.001\n",
    "    best_cross_loss = float('inf')\n",
    "    best_r2 = float('-inf') \n",
    "    best_model_state = None\n",
    "    \n",
    "    # setup the k-fold cross validation\n",
    "    k_folds = 5\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    trues, preds, tprs, fprs = {}, {}, {}, {}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_true = []\n",
    "        train_pred = []\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Training'):\n",
    "            inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_true.append(labels.detach().cpu().numpy())\n",
    "            outputs = (outputs >= 0.5).int()\n",
    "            train_pred.append(outputs.squeeze().detach().cpu().numpy())\n",
    "\n",
    "        # Evaluate on cross-validation set\n",
    "        model.eval()\n",
    "        cross_loss = 0\n",
    "        cross_true = []\n",
    "        cross_pred = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for inputs, labels in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Cross-validation'):\n",
    "                inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                cross_loss += loss.item()\n",
    "                cross_true.append(labels.detach().cpu().numpy())\n",
    "                outputs = (outputs >= 0.5).int()\n",
    "                cross_pred.append(outputs.squeeze().detach().cpu().numpy())\n",
    "\n",
    "            cross_true = np.concatenate(cross_true)\n",
    "            cross_pred = np.concatenate(cross_pred)\n",
    "\n",
    "        # Flatten the collected predictions and true labels\n",
    "        all_true = np.concatenate([cross_true])\n",
    "        all_pred = np.concatenate([cross_pred])\n",
    "\n",
    "        # Compute confusion matrix: [[TN, FP], [FN, TP]]\n",
    "        tn, fp, fn, tp = confusion_matrix(all_true, all_pred).ravel()\n",
    "\n",
    "        # Compute TPR and FPR\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "        # Print statistics\n",
    "        print(f\"\\nEpoch {epoch+1} Cross-validation Statistics:\")\n",
    "        print(f\"Cross-validation Loss: {cross_loss/len(val_loader):.4f}\")\n",
    "        print(f\"TPR (Recall): {tpr:.4f}\")\n",
    "        print(f\"FPR: {fpr:.4f}\")\n",
    "\n",
    "        # Save metrics\n",
    "        trues[epoch] = cross_true\n",
    "        preds[epoch] = cross_pred\n",
    "        tprs[epoch] = tpr\n",
    "        fprs[epoch] = fpr\n",
    "\n",
    "#         Save the best model\n",
    "#         if tpr > best_tpr:  \n",
    "#             best_tpr = tpr\n",
    "#             best_model_state = model.state_dict()\n",
    "\n",
    "#         if cross_loss < best_cross_loss:\n",
    "#             best_cross_loss = cross_loss\n",
    "    \n",
    "    plt.title(f\"lr = {lr}\")\n",
    "    plt.plot(list(range(num_epochs)), list(tprs.values()))\n",
    "    plt.plot(list(range(num_epochs)), list(fprs.values()))\n",
    "    plt.legend([\"TPR\", \"FPR\"])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_kfold(model_class, \n",
    "                dataset,\n",
    "                inps, labels,\n",
    "                device,\n",
    "                lr       = 1e-3,\n",
    "                model_args=None,\n",
    "                k_folds  = 5,\n",
    "                num_epochs = 50,\n",
    "                batch_size = 64):\n",
    "    \n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_tpr, fold_fpr = [], []\n",
    "    best_model_state, best_tpr = None, -np.inf\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X=inps, y=labels)):\n",
    "        print(f\"\\n=== Fold {fold+1}/{k_folds} ===\")\n",
    "        \n",
    "        train_loader = DataLoader(Subset(dataset, train_idx),\n",
    "                                  batch_size=batch_size, shuffle=True)\n",
    "        val_loader   = DataLoader(Subset(dataset, val_idx),\n",
    "                                  batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Fresh model & optimiser\n",
    "        model = model_class(*model_args).to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        # Per-epoch metric history for this fold\n",
    "        tpr_hist, fpr_hist = [], []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for inputs, labels in tqdm(train_loader,\n",
    "                                       desc=f\"Fold {fold+1} | Epoch {epoch+1}/{num_epochs} - train\",\n",
    "                                       leave=False):\n",
    "                inputs  = inputs.to(device).float()\n",
    "                labels  = labels.to(device).float()\n",
    "                outputs = model(inputs)\n",
    "                loss    = criterion(outputs.squeeze(), labels)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            cross_true, cross_pred = [], []\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in tqdm(val_loader,\n",
    "                                           desc=f\"Fold {fold+1} | Epoch {epoch+1}/{num_epochs} - val\",\n",
    "                                           leave=False):\n",
    "                    inputs  = inputs.to(device).float()\n",
    "                    labels  = labels.to(device).float()\n",
    "                    outputs = model(inputs)\n",
    "                    preds   = (outputs >= 0.5).int()\n",
    "                    \n",
    "                    cross_true.append(labels.cpu().numpy().ravel())\n",
    "                    cross_pred.append(preds.cpu().numpy().ravel())\n",
    "            \n",
    "            # Flatten for confusion-matrix\n",
    "            y_true = np.concatenate(cross_true)\n",
    "            y_pred = np.concatenate(cross_pred)\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "            tpr = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "            fpr = fp / (fp + tn) if (fp + tn) else 0.0\n",
    "            tpr_hist.append(tpr)\n",
    "            fpr_hist.append(fpr)\n",
    "            \n",
    "            print(f\"  Epoch {epoch+1}: TPR={tpr:.4f}, FPR={fpr:.4f}\")\n",
    "            \n",
    "            # Keep best model across ALL folds\n",
    "            if tpr > best_tpr:\n",
    "                best_tpr = tpr\n",
    "                best_model_state = model.state_dict()\n",
    "        \n",
    "        fold_tpr.append(tpr_hist[-1])\n",
    "        fold_fpr.append(fpr_hist[-1])\n",
    "        \n",
    "        plt.figure(figsize=(5,3))\n",
    "        plt.plot(range(1, num_epochs+1), tpr_hist, label=\"TPR\")\n",
    "        plt.plot(range(1, num_epochs+1), fpr_hist, label=\"FPR\")\n",
    "        plt.title(f\"Fold {fold+1} | lr={lr}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.legend(); plt.tight_layout(); plt.show()\n",
    "    \n",
    "    print(\"Cross-validation summary\")\n",
    "    print(f\"Mean  TPR: {np.mean(fold_tpr):.4f}  ± {np.std(fold_tpr):.4f}\")\n",
    "    print(f\"Mean  FPR: {np.mean(fold_fpr):.4f}  ± {np.std(fold_fpr):.4f}\")\n",
    "    \n",
    "\n",
    "        \n",
    "    return best_model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def evaluate_auroc(model, data_loader, device, results_df=None, model_name=\"Model\"):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            preds = outputs.squeeze().cpu().numpy() # probability values for each in [0,1]\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    output = (np.array(all_preds) >= 0.5).astype(int) # thresholding to convert to 0s and 1s\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, output)\n",
    "    precision = precision_score(all_labels, output, zero_division=0)\n",
    "    recall = recall_score(all_labels, output, zero_division=0)\n",
    "    f1 = f1_score(all_labels, output, zero_division=0)\n",
    "    try:\n",
    "        auroc = roc_auc_score(all_labels, all_preds) # the only one we actually want preds from\n",
    "    except ValueError:\n",
    "        auroc = float('nan')  # in case only one class in y_true\n",
    "\n",
    "    # Prepare results\n",
    "    result_row = pd.DataFrame([{\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1,\n",
    "        'AUROC': auroc\n",
    "    }])\n",
    "\n",
    "    # Append to existing DataFrame\n",
    "    if results_df is None:\n",
    "        results_df = result_row\n",
    "    else:\n",
    "        results_df = pd.concat([results_df, result_row], ignore_index=True)\n",
    "\n",
    "    return all_labels, all_preds, results_df\n",
    "\n",
    "def plot_auroc(labels, probs, threshold=0.5):\n",
    "    fpr, tpr, thresholds = roc_curve(labels, probs)\n",
    "    auc = roc_auc_score(labels, probs)\n",
    "\n",
    "    # Find the closest threshold index to 0.5\n",
    "    idx = np.argmin(np.abs(thresholds - threshold))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'AUROC = {auc:.4f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # random guess\n",
    "    plt.scatter(fpr[idx], tpr[idx], color='red', label=f'Threshold = {threshold}')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('AUROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "janus_inp_dim = 8\n",
    "janus_out_dim = 1\n",
    "janus_mod_dims = [8,16,32,64,128]\n",
    "janus_lr = [10**(-4),10**(-3)]\n",
    "janus_weight_decay = [0, 0.01]\n",
    "\n",
    "torch_results_df = pd.DataFrame()\n",
    "\n",
    "for janus_mod_dim in janus_mod_dims:\n",
    "    for lr in janus_lr:\n",
    "        model_state = train_kfold(Janus, train_val_dataset.dataset, train_and_val_inps, train_val_labels, device, lr, model_args=(janus_inp_dim, janus_out_dim, janus_mod_dim))\n",
    "        model = Janus(janus_inp_dim, janus_out_dim, janus_mod_dim)\n",
    "        model.load_state_dict(model_state)\n",
    "        model.to(device)\n",
    "        labels, preds, torch_results_df = evaluate_auroc(model, test_loader, device, torch_results_df, model_name=f\"Janus Dim:{janus_mod_dim}, LR:{lr}\")\n",
    "        plot_auroc(labels, preds)\n",
    "        \n",
    "display(torch_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_results_df.to_csv('0820_fullseq_lyra_classifications.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, bidirectional=False):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size * self.num_directions, output_size)\n",
    "        self.sigout = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers * self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers * self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: (batch_size, seq_len, hidden_size * num_directions)\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.sigout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 8      # Number of input features\n",
    "hidden_sizes = np.\n",
    "num_layers_search = [1,5,10]       # Number of LSTM layers\n",
    "bidirectional = [False, True]\n",
    "output_size = 1      # Output size\n",
    "\n",
    "lstm_results_df = pd.DataFrame()\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    for num_layers in num_layers_search:\n",
    "            for direction in bidirectional:\n",
    "                model_state = train_kfold(LSTMModel, train_val_dataset.dataset, train_and_val_inps, train_val_labels, device, lr, model_args=(input_size, hidden_size, num_layers, output_size, direction))\n",
    "                model = LSTMModel(input_size, hidden_size, num_layers, output_size, direction)\n",
    "                model.load_state_dict(model_state)\n",
    "                model.to(device)\n",
    "                labels, preds, lstm_results_df = evaluate_auroc(model, test_loader, device, torch_results_df, model_name=f\"LSTM Hidden Units:{hidden_size}, Num Layers:{num_layers}, Bidirectional:{direction}\")\n",
    "                plot_auroc(labels, preds)\n",
    "\n",
    "            \n",
    "display(lstm_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, output_dim=2, num_layers=1, dropout=0.1, max_len=1500):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.positional_encoding = self._generate_positional_encoding(model_dim, max_len)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim, nhead=num_heads,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_layer = nn.Linear(model_dim, output_dim)  \n",
    "\n",
    "    def _generate_positional_encoding(self, d_model, max_len):\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        return pe  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.positional_encoding[:, :x.size(1)].to(x.device)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1) \n",
    "        return self.output_layer(x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def train_loop_tf(model, lr):\n",
    "    metrics = []\n",
    "    num_epochs = 50\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    best_cross_loss = float('inf')\n",
    "\n",
    "    trues, preds, tprs, fprs = {}, {}, {}, {}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_true = []\n",
    "        train_pred = []\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Training'):\n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device).long()  # class indices: 0 or 1\n",
    "\n",
    "            outputs = model(inputs)  # [batch, 2]\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_true.append(labels.detach().cpu().numpy())\n",
    "            pred_labels = torch.argmax(outputs, dim=1)\n",
    "            train_pred.append(pred_labels.detach().cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        cross_loss = 0\n",
    "        cross_true = []\n",
    "        cross_pred = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Cross-validation'):\n",
    "                inputs = inputs.to(device).float()\n",
    "                labels = labels.to(device).long()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                cross_loss += loss.item()\n",
    "\n",
    "                cross_true.append(labels.detach().cpu().numpy())\n",
    "                pred_labels = torch.argmax(outputs, dim=1)\n",
    "                cross_pred.append(pred_labels.detach().cpu().numpy())\n",
    "\n",
    "        cross_true = np.concatenate(cross_true)\n",
    "        cross_pred = np.concatenate(cross_pred)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(cross_true, cross_pred).ravel()\n",
    "\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1} Cross-validation Statistics:\")\n",
    "        print(f\"Cross-validation Loss: {cross_loss/len(val_loader):.4f}\")\n",
    "        print(f\"TPR (Recall): {tpr:.4f}\")\n",
    "        print(f\"FPR: {fpr:.4f}\")\n",
    "\n",
    "        trues[epoch] = cross_true\n",
    "        preds[epoch] = cross_pred\n",
    "        tprs[epoch] = tpr\n",
    "        fprs[epoch] = fpr\n",
    "\n",
    "    plt.title(f\"lr = {lr}\")\n",
    "    plt.plot(list(range(num_epochs)), list(tprs.values()))\n",
    "    plt.plot(list(range(num_epochs)), list(fprs.values()))\n",
    "    plt.legend([\"TPR\", \"FPR\"])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_auroc_tf(model, data_loader, device, results_df=None, model_name=\"Model\"):\n",
    "    model.eval()\n",
    "    all_probs = []   # soft predictions\n",
    "    all_preds = []   # hard predictions\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()  \n",
    "            preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "            all_probs.extend(probs)\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute metrics using thresholded predictions\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "    try:\n",
    "        auroc = roc_auc_score(all_labels, all_probs)\n",
    "    except ValueError:\n",
    "        auroc = float('nan')  # if only one class present\n",
    "\n",
    "    result_row = pd.DataFrame([{\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1,\n",
    "        'AUROC': auroc\n",
    "    }])\n",
    "\n",
    "    if results_df is None:\n",
    "        results_df = result_row\n",
    "    else:\n",
    "        results_df = pd.concat([results_df, result_row], ignore_index=True)\n",
    "\n",
    "    return all_labels, all_probs, results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = 10\n",
    "model_dims = [64]\n",
    "num_heads_list = [2, 4, 8, 16, 32]\n",
    "num_layers_list = [2, 4, 8, 16]\n",
    "num_classes = 2  # e.g., binary classification\n",
    "max_seq_len = 1421\n",
    "# max_seq_len = 56 # change between either full or core-seq representations\n",
    "\n",
    "tf_df = pd.DataFrame()\n",
    "\n",
    "for model_dim in model_dims:\n",
    "    for num_heads in num_heads_list:\n",
    "        for num_layers in num_layers_list:\n",
    "            model = SimpleTransformer(input_dim=vocab_size, model_dim=model_dim, num_heads=num_heads, output_dim=num_classes,\n",
    "                                     num_layers=num_layers, max_len=max_seq_len)\n",
    "            model.to(device)\n",
    "            train_loop_tf(model, lr=lr)\n",
    "            labels, preds, tf_df = evaluate_auroc_tf(model, test_loader, device, tf_df, model_name=f\"Transformer, model dim:{model_dim}, num_heads:{num_heads}, num_layers:{num_layers}\")\n",
    "            plot_auroc(labels, preds)\n",
    "\n",
    "display(tf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_df.to_csv(\"0820_fullseq_transformer_classifications.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from Green et al. 2022\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # input: (batch_size, 8, 56)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=10, out_channels=64, kernel_size=4)\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size=12)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=3)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(64, 32, kernel_size=3)\n",
    "        self.conv4 = nn.Conv1d(32, 32, kernel_size=3)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=3)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # flatten out linear input size\n",
    "        dummy = torch.zeros(1, 1421, 10)\n",
    "        with torch.no_grad():\n",
    "            dummy_out = self._forward_conv(dummy.permute(0, 2, 1))\n",
    "            flat_size = dummy_out.numel()\n",
    "\n",
    "        self.dense1 = nn.Linear(flat_size, 256)\n",
    "        self.dense2 = nn.Linear(256, 256)\n",
    "        self.output = nn.Linear(256, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def _forward_conv(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 56, 8) to (batch_size, 8, 56)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self._forward_conv(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        x = F.relu(self.dense2(x))\n",
    "        return self.sigmoid(self.output(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "model.to(device)\n",
    "train_loop(model, lr=0.00005)\n",
    "labels, preds, _ = evaluate_auroc(model, test_loader, device, pd.DataFrame(), model_name=f\"fff\")\n",
    "plot_auroc(labels, preds)\n",
    "display(_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
