{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from einops import rearrange, repeat\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm.auto import tqdm\n",
    "from Bio import Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "font_files = fm.findSystemFonts(fontpaths='/home/jupyter/ADAPT_PCR_share/safe/resources/Helvetica')\n",
    "for font_file in font_files:\n",
    "    fm.fontManager.addfont(font_file)\n",
    "mpl.rcParams['font.sans-serif'] = 'Helvetica'\n",
    "mpl.rcParams['axes.spines.right'] = 'off'\n",
    "mpl.rcParams['axes.spines.top'] = 'off'\n",
    "mpl.rcParams['figure.figsize'] = (2.5,2.5)\n",
    "mpl.rcParams['axes.labelsize']: '10'\n",
    "mpl.rcParams['xtick.labelsize']: '10'\n",
    "mpl.rcParams['ytick.labelsize']: '10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEPATH = '/home/jupyter/ADAPT_PCR_share/safe/results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/home/jupyter/ADAPT_PCR_share/safe/dataset/0717_dataset_train.csv',index_col=[0,1])\n",
    "valid_df = pd.read_csv('/home/jupyter/ADAPT_PCR_share/safe/dataset/0717_dataset_valid.csv',index_col=[0,1])\n",
    "test_df = pd.read_csv('/home/jupyter/ADAPT_PCR_share/safe/dataset/0717_dataset_test.csv',index_col=[0,1])\n",
    "\n",
    "print(train_df.shape, valid_df.shape, test_df.shape)\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = train_df['score'].tolist() + test_df['score'].tolist() + valid_df['score'].tolist()\n",
    "print(len(vs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = list(np.arange(0,1.11,.05)-.025) + [2]\n",
    "ys, ts = np.histogram(vs, bins=bins)\n",
    "xs = np.arange(0,1.11,.05)\n",
    "plt.bar(xs,ys,width=.05,color='gray')\n",
    "plt.xlabel('Activity score')\n",
    "plt.ylabel('PCR reactions')\n",
    "plt.savefig(SAVEPATH + '0828_hist.png',bbox_inches='tight',dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature-based regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = ['f_length','f_Tm','f_GC','f_indel','f_mm','r_length','r_Tm','r_GC','r_indel','r_mm','prod_length','prod_Tm']\n",
    "newfs = ['f_len','f_Tm','f_GC','f_indel','f_mm','r_len','r_Tm','r_GC','r_indel','r_mm','prod_len','prod_Tm']\n",
    "\n",
    "train_feats = train_df[fs]\n",
    "train_feats.columns = newfs\n",
    "valid_feats = valid_df[fs]\n",
    "test_feats = test_df[fs]\n",
    "print(train_feats.shape, valid_feats.shape, test_feats.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_feats)\n",
    "\n",
    "y_train = train_df['score'].values\n",
    "y_val = valid_df['score'].values\n",
    "y_test = test_df['score'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "rf_val_pred = rf_model.predict(X_val)\n",
    "rf_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "rf_val_r2 = r2_score(y_val, rf_val_pred)\n",
    "rf_test_r2 = r2_score(y_test, rf_test_pred)\n",
    "\n",
    "print(\"Random Forest:\")\n",
    "print(f\"  Validation R2: {rf_val_r2:.4f}\")\n",
    "print(f\"  Test R2: {rf_test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,1.8))\n",
    "plt.bar(range(len(fs)),rf_model.feature_importances_)\n",
    "plt.xticks(range(len(fs)),fs,rotation=90)\n",
    "plt.ylabel('Feature importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "svr_model.fit(X_train, y_train)\n",
    "\n",
    "svr_val_pred = svr_model.predict(X_val)\n",
    "svr_test_pred = svr_model.predict(X_test)\n",
    "\n",
    "svr_val_r2 = r2_score(y_val, svr_val_pred)\n",
    "svr_test_r2 = r2_score(y_test, svr_test_pred)\n",
    "\n",
    "print(\"SVR:\")\n",
    "print(f\"  Validation R2: {svr_val_r2:.4f}\")\n",
    "print(f\"  Test R2: {svr_test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "mlp_val_pred = mlp_model.predict(X_val)\n",
    "mlp_test_pred = mlp_model.predict(X_test)\n",
    "\n",
    "mlp_val_r2 = r2_score(y_val, mlp_val_pred)\n",
    "mlp_test_r2 = r2_score(y_test, mlp_test_pred)\n",
    "\n",
    "print(\"MLPRegressor:\")\n",
    "print(f\"  Validation R2: {mlp_val_r2:.4f}\")\n",
    "print(f\"  Test R2: {mlp_test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "lr_val_pred = lr_model.predict(X_val)\n",
    "lr_test_pred = lr_model.predict(X_test)\n",
    "\n",
    "lr_val_r2 = r2_score(y_val, lr_val_pred)\n",
    "lr_test_r2 = r2_score(y_test, lr_test_pred)\n",
    "\n",
    "print(\"Linear Regression:\")\n",
    "print(f\"  Validation R2: {lr_val_r2:.4f}\")\n",
    "print(f\"  Test R2: {lr_test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = Ridge(alpha=1000)  # alpha controls the regularization strength\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "ridge_val_pred = ridge_model.predict(X_val)\n",
    "ridge_test_pred = ridge_model.predict(X_test)\n",
    "\n",
    "ridge_val_r2 = r2_score(y_val, ridge_val_pred)\n",
    "ridge_test_r2 = r2_score(y_test, ridge_test_pred)\n",
    "\n",
    "print(\"Ridge Regression:\")\n",
    "print(f\"  Validation R2: {ridge_val_r2:.4f}\")\n",
    "print(f\"  Test R2: {ridge_test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gbr_model.fit(X_train, y_train)\n",
    "\n",
    "gbr_val_pred = gbr_model.predict(X_val)\n",
    "gbr_test_pred = gbr_model.predict(X_test)\n",
    "\n",
    "gbr_val_r2 = r2_score(y_val, gbr_val_pred)\n",
    "gbr_test_r2 = r2_score(y_test, gbr_test_pred)\n",
    "\n",
    "print(\"Gradient Boosting Regressor:\")\n",
    "print(f\"  Validation R2: {gbr_val_r2:.4f}\")\n",
    "print(f\"  Test R2: {gbr_test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,1.8))\n",
    "plt.bar(range(len(fs)),gbr_model.feature_importances_)\n",
    "plt.xticks(range(len(fs)),fs,rotation=90)\n",
    "plt.ylabel('Feature importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = ['Linear','Ridge','RF','SVR','MLP','GBR']\n",
    "all_preds = [ lr_test_pred, ridge_test_pred, rf_test_pred, svr_test_pred, mlp_test_pred, gbr_test_pred ]\n",
    "tbl = pd.DataFrame(index=all_labels,columns=['R2','MAE','RMSE'])\n",
    "for label,y_pred in zip(all_labels,all_preds):\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    tbl.loc[label] = pd.Series({'R2':r2,'MAE':mae,'RMSE':rmse})\n",
    "tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.\n",
    "wd = 0.\n",
    "otherhp = 'na'\n",
    "ds = 'Test'\n",
    "enc = '12fs'\n",
    "epoch = 1\n",
    "t = '%.4f'%0\n",
    "\n",
    "for label,y_pred in zip(all_labels,all_preds):\n",
    "    r2 = '%.4f'%r2_score(y_test, y_pred)\n",
    "    mae = '%.4f'%mean_absolute_error(y_test, y_pred)\n",
    "    rmse = '%.4f'%mean_squared_error(y_test, y_pred, squared=False)\n",
    "    row = [label, lr, wd, otherhp, ds, enc, epoch, r2, mae, rmse, t]\n",
    "    with open(OUTFILE,'a') as out:\n",
    "        out.write('\\t'.join(map(str,row))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head $OUTFILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from einops import rearrange, repeat\n",
    "from tqdm.auto import tqdm\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(seq, length=28):\n",
    "    mapping = { 'A':[1, 0, 0, 0, 0],\n",
    "                'T':[0, 1, 0, 0, 0],\n",
    "                'C':[0, 0, 1, 0, 0],\n",
    "                'G':[0, 0, 0, 1, 0],\n",
    "                'N':[0, 0, 0, 0, 0],\n",
    "                '-':[0, 0, 0, 0, 1] }\n",
    "    seq = seq.ljust(length, 'N') # (6, ATCG) -> NNATCG\n",
    "    return np.array([mapping[char.upper()] for char in seq])\n",
    "\n",
    "def one_hot_encode_full_gap(df_seqs, maxl=1421):\n",
    "    primer_encoded = []\n",
    "    target_encoded = []\n",
    "    for (tname,pname),row in df_seqs.iterrows():\n",
    "        fseq, fst, rseq, rst, tseq = row[['f_seq','f_start','r_seq','r_start','target_seq']]\n",
    "        fenc, ftenc, renc, rtenc = row[['f_penc','f_tenc','r_penc','r_tenc']]\n",
    "        pseq = 'N'*fst + fenc + 'N'*(rst-(fst+len(fseq))) + renc + 'N'*(len(tseq)-(rst+len(rseq)))\n",
    "        tseq = tseq[:fst] + ftenc + tseq[fst+len(fseq):rst] + rtenc + tseq[rst+len(rseq):]\n",
    "        primer_encoded.append(one_hot_encode(pseq, maxl))\n",
    "        target_encoded.append(one_hot_encode(tseq, maxl))\n",
    "    final_encoded = np.append(np.array(target_encoded), np.array(primer_encoded), axis=2)\n",
    "    print(final_encoded.shape)\n",
    "    return torch.tensor(final_encoded, dtype=torch.float32)\n",
    "\n",
    "def one_hot_encode_pbs_gap(df_seqs):\n",
    "    primer_encoded = []\n",
    "    target_encoded = []\n",
    "    for (tname,pname),row in df_seqs.iterrows():\n",
    "        fenc, ftenc, renc, rtenc = row[['f_penc','f_tenc','r_penc','r_tenc']].apply(one_hot_encode)\n",
    "        prienc = np.append(fenc,renc,axis=0)\n",
    "        tarenc = np.append(ftenc,rtenc,axis=0)\n",
    "        primer_encoded.append(prienc)\n",
    "        target_encoded.append(tarenc)\n",
    "    primer_encoded = np.array(primer_encoded)\n",
    "    target_encoded = np.array(target_encoded)\n",
    "    final_encoded = np.append(target_encoded, primer_encoded, axis=2)\n",
    "    print(final_encoded.shape)\n",
    "    return torch.tensor(final_encoded, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PcrDataset(Dataset):\n",
    "    def __init__(self, encoded_input, ct_values):\n",
    "        self.encoded_input = encoded_input\n",
    "        self.ct_values = ct_values\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_input)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_input[idx], self.ct_values[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = one_hot_encode_full_gap(train_df)\n",
    "X_val = one_hot_encode_full_gap(valid_df)\n",
    "X_test = one_hot_encode_full_gap(test_df)\n",
    "\n",
    "y_train = train_df['score'].values\n",
    "y_val = valid_df['score'].values\n",
    "y_test = test_df['score'].values\n",
    "\n",
    "data_train = PcrDataset(X_train, y_train)\n",
    "data_val = PcrDataset(X_val, y_val)\n",
    "data_test = PcrDataset(X_test, y_test)\n",
    "\n",
    "loader_train = DataLoader(data_train, batch_size=64, shuffle=True)\n",
    "loader_val = DataLoader(data_val, batch_size=64, shuffle=False)\n",
    "loader_test = DataLoader(data_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "input_dim = 10\n",
    "output_dim = 1\n",
    "learning_rates = [10**-3, 10**-4]\n",
    "weight_decays = [10**-2, 10**-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lyra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGC(nn.Module):\n",
    "    def __init__(self,model_dim,expansion_factor = 1.0,dropout = 0.0):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.dropout = dropout\n",
    "        self.conv = nn.Conv1d(int(model_dim * expansion_factor), int(model_dim * expansion_factor),\n",
    "                              kernel_size=3, padding=1, groups=int(model_dim * expansion_factor))\n",
    "        self.in_proj = nn.Linear(model_dim, int(model_dim * expansion_factor * 2))\n",
    "        self.out_norm = nn.RMSNorm(int(model_dim), eps=1e-8)\n",
    "        self.in_norm = nn.RMSNorm(int(model_dim * expansion_factor * 2), eps=1e-8)\n",
    "        self.out_proj = nn.Linear(int(model_dim * expansion_factor), model_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, u):\n",
    "        xv = self.in_norm(self.in_proj(u))\n",
    "        x,v = xv.chunk(2,dim=-1)\n",
    "        x_conv = self.conv(x.transpose(-1,-2)).transpose(-1,-2)\n",
    "        gate =  v * x_conv\n",
    "        x = self.out_norm(self.out_proj(gate))\n",
    "        return x\n",
    "    \n",
    "class DropoutNd(nn.Module):\n",
    "    def __init__(self, p: float = 0.5, tie=True, transposed=True):\n",
    "        \"\"\"\n",
    "        tie: tie dropout mask across sequence lengths (Dropout1d/2d/3d)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if p < 0 or p >= 1:\n",
    "            raise ValueError(\"dropout probability has to be in [0, 1), \" \"but got {}\".format(p))\n",
    "        self.p = p\n",
    "        self.tie = tie\n",
    "        self.transposed = transposed\n",
    "        self.binomial = torch.distributions.binomial.Binomial(probs=1-self.p)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"X: (batch, dim, lengths...).\"\"\"\n",
    "        if self.training:\n",
    "            if not self.transposed: X = rearrange(X, 'b ... d -> b d ...')\n",
    "            # binomial = torch.distributions.binomial.Binomial(probs=1-self.p) \n",
    "            # This is incredibly slow because of CPU -> GPU copying\n",
    "            mask_shape = X.shape[:2] + (1,)*(X.ndim-2) if self.tie else X.shape\n",
    "            # mask = self.binomial.sample(mask_shape)\n",
    "            mask = torch.rand(*mask_shape, device=X.device) < 1.-self.p\n",
    "            X = X * mask * (1.0/(1-self.p))\n",
    "            if not self.transposed: X = rearrange(X, 'b d ... -> b ... d')\n",
    "            return X\n",
    "        return X\n",
    "\n",
    "class S4DKernel(nn.Module):\n",
    "    \"\"\"Generate convolution kernel from diagonal SSM parameters.\"\"\"\n",
    "\n",
    "    def __init__(self, model_dim, N=64, dt_min=0.001, dt_max=0.1, lr=None):\n",
    "        super().__init__()\n",
    "        # Generate dt\n",
    "        H = model_dim\n",
    "        log_dt = torch.rand(H) * (\n",
    "            math.log(dt_max) - math.log(dt_min)\n",
    "        ) + math.log(dt_min)\n",
    "\n",
    "        C = torch.randn(H, N // 2, dtype=torch.cfloat)\n",
    "        self.C = nn.Parameter(torch.view_as_real(C))\n",
    "        self.register(\"log_dt\", log_dt, lr)\n",
    "\n",
    "        log_A_real = torch.log(0.5 * torch.ones(H, N//2))\n",
    "        A_imag = math.pi * repeat(torch.arange(N//2), 'n -> h n', h=H)\n",
    "        self.register(\"log_A_real\", log_A_real, lr)\n",
    "        self.register(\"A_imag\", A_imag, lr)\n",
    "\n",
    "    def forward(self, L):\n",
    "        \"\"\"\n",
    "        returns: (..., c, L) where c is number of channels (default 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # Materialize parameters\n",
    "        dt = torch.exp(self.log_dt) # (H)\n",
    "        C = torch.view_as_complex(self.C) # (H N)\n",
    "        A = -torch.exp(self.log_A_real) + 1j * self.A_imag # (H N)\n",
    "\n",
    "        # Vandermonde multiplication\n",
    "        dtA = A * dt.unsqueeze(-1)  # (H N)\n",
    "        K = dtA.unsqueeze(-1) * torch.arange(L, device=A.device) # (H N L)\n",
    "        C = C * (torch.exp(dtA)-1.) / A\n",
    "        K = 2 * torch.einsum('hn, hnl -> hl', C, torch.exp(K)).real\n",
    "\n",
    "        return K\n",
    "\n",
    "    def register(self, name, tensor, lr=None):\n",
    "        \"\"\"Register a tensor with a configurable learning rate and 0 weight decay\"\"\"\n",
    "\n",
    "        if lr == 0.0:\n",
    "            self.register_buffer(name, tensor)\n",
    "        else:\n",
    "            self.register_parameter(name, nn.Parameter(tensor))\n",
    "\n",
    "            optim = {\"weight_decay\": 0.0}\n",
    "            if lr is not None: optim[\"lr\"] = lr\n",
    "            setattr(getattr(self, name), \"_optim\", optim)\n",
    "\n",
    "\n",
    "class S4D(nn.Module):\n",
    "    def __init__(self, model_dim, state_dim=64, dropout=0.0, transposed=True, **kernel_args):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h = model_dim\n",
    "        self.n = state_dim\n",
    "        self.output_dim = self.h\n",
    "        self.transposed = transposed\n",
    "\n",
    "        self.D = nn.Parameter(torch.randn(self.h))\n",
    "        # SSM Kernel\n",
    "        self.kernel = S4DKernel(self.h, N=self.n, **kernel_args)\n",
    "        # Pointwise\n",
    "        self.activation = nn.GELU()\n",
    "        dropout_fn = DropoutNd\n",
    "        self.dropout = dropout_fn(dropout) if dropout > 0.0 else nn.Identity()\n",
    "\n",
    "        # position-wise output transform to mix features\n",
    "        self.output_linear = nn.Sequential(\n",
    "            nn.Conv1d(self.h, 2*self.h, kernel_size=1),\n",
    "            nn.GLU(dim=-2),\n",
    "        )\n",
    "\n",
    "    def forward(self, u, **kwargs): # absorbs return_output and transformer src mask\n",
    "        \"\"\" Input and output shape (B, H, L) \"\"\"\n",
    "        if not self.transposed: u = u.transpose(-1, -2)\n",
    "        L = u.size(-1)\n",
    "        # Compute SSM Kernel\n",
    "        k = self.kernel(L=L) # (H L)\n",
    "\n",
    "        # Convolution\n",
    "        k_f = torch.fft.rfft(k, n=2*L)  # (H L)\n",
    "        u_f = torch.fft.rfft(u, n=2*L) # (B H L)\n",
    "        y = torch.fft.irfft(u_f*k_f, n=2*L)[..., :L] # (B H L)\n",
    "\n",
    "        # Compute D term in state space equation - essentially a skip connection\n",
    "        y = y + u * self.D.unsqueeze(-1)\n",
    "\n",
    "        y = self.dropout(self.activation(y))\n",
    "        y = self.output_linear(y)\n",
    "        if not self.transposed: y = y.transpose(-1, -2)\n",
    "        return y\n",
    "    \n",
    "class Janus(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, model_dim, state_dim=64, dropout=0.2, transposed=False, **kernel_args):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_dim, model_dim)\n",
    "        self.pgc1 = PGC(model_dim, expansion_factor=0.25, dropout=dropout)\n",
    "        self.pgc2 = PGC(model_dim, expansion_factor=2, dropout=dropout)\n",
    "        self.s4d = S4D(model_dim, state_dim=state_dim, dropout=dropout, transposed=transposed, **kernel_args)\n",
    "        self.norm = nn.RMSNorm(model_dim)\n",
    "        self.decoder = nn.Linear(model_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, u):\n",
    "        x = self.encoder(u)\n",
    "        x = self.pgc1(x)\n",
    "        x = self.pgc2(x)\n",
    "        z = x\n",
    "        z = self.norm(z)\n",
    "        x = self.dropout(self.s4d(z)) + x\n",
    "        x = x.mean(dim=1)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dims = [64,128]\n",
    "conds = [(lr,wd,md) for lr in learning_rates for wd in weight_decays for md in model_dims]\n",
    "print(len(conds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     7
    ]
   },
   "outputs": [],
   "source": [
    "for lr, wd, model_dim in conds:\n",
    "    model = Janus(input_dim=input_dim, output_dim=output_dim, model_dim=model_dim).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd) \n",
    "    best_state = None\n",
    "    best_r2 = float('-inf')\n",
    "    otherhp = 'model_dim=%i'%model_dim\n",
    "    ## Training\n",
    "    for epoch in tqdm(range(num_epochs), desc='LR %s; WD %s; MD %s, Epoch'%(lr,wd,model_dim)):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        for inputs, labels in loader_train:\n",
    "            inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_time = (time.time() - start)\n",
    "        \n",
    "        model.eval()\n",
    "        cross_loss = 0\n",
    "        cross_true, cross_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in loader_val:\n",
    "                inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                cross_loss += loss.item()\n",
    "                cross_true.append(labels.detach().cpu().numpy())\n",
    "                cross_pred.append(outputs.squeeze().detach().cpu().numpy())\n",
    "            cross_true = np.concatenate(cross_true)\n",
    "            cross_pred = np.concatenate(cross_pred)\n",
    "\n",
    "        epoch_r2 = r2_score(cross_true, cross_pred)\n",
    "        #print('%i\\t%.3f'%(epoch,epoch_r2))\n",
    "        \n",
    "        mae = '%.4f'%mean_absolute_error(cross_true, cross_pred)\n",
    "        rmse = '%.4f'%mean_squared_error(cross_true, cross_pred, squared=False)\n",
    "        row = ['Lyra', lr, wd, otherhp, 'Val', 'PBS', epoch, '%.4f'%epoch_r2, mae, rmse, '%.4f'%train_time]\n",
    "        with open(OUTFILE,'a') as out:\n",
    "            out.write('\\t'.join(map(str,row))+'\\n')\n",
    "        \n",
    "        if epoch_r2 > best_r2:\n",
    "            best_r2 = epoch_r2\n",
    "            best_state = model.state_dict()\n",
    "\n",
    "    ## Test\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    test_true = []\n",
    "    test_pred = []\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader_test:\n",
    "            inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            test_true.append(labels.detach().cpu().numpy())\n",
    "            test_pred.append(outputs.squeeze().detach().cpu().numpy())\n",
    "        test_true = np.concatenate(test_true)\n",
    "        test_pred = np.concatenate(test_pred)\n",
    "\n",
    "    infer_time = (time.time() - start)\n",
    "    val_r2 = best_r2\n",
    "    test_r2 = r2_score(test_true, test_pred)\n",
    "    \n",
    "    mae = '%.4f'%mean_absolute_error(test_true, test_pred)\n",
    "    rmse = '%.4f'%mean_squared_error(test_true, test_pred, squared=False)\n",
    "    row = ['Lyra', lr, wd, otherhp, 'Test', 'PBS', 1, '%.4f'%test_r2, mae, rmse, '%.4f'%infer_time]\n",
    "    with open(OUTFILE,'a') as out:\n",
    "        out.write('\\t'.join(map(str,row))+'\\n')\n",
    "    \n",
    "    print(\"Lyra; LR %s; WD %s; MD %s\"%(lr,wd,model_dim))\n",
    "    print(f\"  Validation R2: {val_r2:.4f}\")\n",
    "    print(f\"  Test R2: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from Green et al. 2022\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, input_len, lin_dim, output_dim):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # input: (batch_size, 8, 56)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=4)\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size=12)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=3)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(64, 32, kernel_size=3)\n",
    "        self.conv4 = nn.Conv1d(32, 32, kernel_size=3)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=3)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # flatten out linear input size\n",
    "        dummy = torch.zeros(1, input_dim, input_len)\n",
    "        with torch.no_grad():\n",
    "            dummy_out = self._forward_conv(dummy)\n",
    "            flat_size = dummy_out.numel()\n",
    "\n",
    "        self.dense1 = nn.Linear(flat_size, lin_dim) # 256 -> 64\n",
    "        self.dense2 = nn.Linear(lin_dim, lin_dim)\n",
    "        self.output = nn.Linear(lin_dim, 1)\n",
    "        # self.sigmoid = nn.Sigmoid() # deleted in regression\n",
    "\n",
    "    def _forward_conv(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 56, 10) to (batch_size, 10, 56)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self._forward_conv(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        x = F.relu(self.dense2(x))\n",
    "        return self.output(x) # self.sigmoid(self.output(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = 1421 # 56\n",
    "lin_dims = [64,128]\n",
    "conds = [(lr,wd,ld) for lr in learning_rates for wd in weight_decays for ld in lin_dims]\n",
    "print(len(conds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     7
    ]
   },
   "outputs": [],
   "source": [
    "for lr, wd, lin_dim in conds:\n",
    "    model = CNN(input_dim=input_dim, input_len=input_len, lin_dim=lin_dim, output_dim=output_dim).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd) \n",
    "    best_state = None\n",
    "    best_r2 = float('-inf')\n",
    "    otherhp = 'linear_dim=%i'%lin_dim\n",
    "    ## Training\n",
    "    for epoch in tqdm(range(num_epochs), desc='LR %s; WD %s; LD %s, Epoch'%(lr,wd,lin_dim)):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        for inputs, labels in loader_train:\n",
    "            inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_time = (time.time() - start)\n",
    "        \n",
    "        model.eval()\n",
    "        cross_loss = 0\n",
    "        cross_true, cross_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in loader_val:\n",
    "                inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                cross_loss += loss.item()\n",
    "                cross_true.append(labels.detach().cpu().numpy())\n",
    "                cross_pred.append(outputs.squeeze().detach().cpu().numpy())\n",
    "            cross_true = np.concatenate(cross_true)\n",
    "            cross_pred = np.concatenate(cross_pred)\n",
    "\n",
    "        epoch_r2 = r2_score(cross_true, cross_pred)\n",
    "        #print('%i\\t%.3f'%(epoch,epoch_r2))\n",
    "        \n",
    "        mae = '%.4f'%mean_absolute_error(cross_true, cross_pred)\n",
    "        rmse = '%.4f'%mean_squared_error(cross_true, cross_pred, squared=False)\n",
    "        row = ['CNN', lr, wd, otherhp, 'Val', 'PBS', epoch, '%.4f'%epoch_r2, mae, rmse, '%.4f'%train_time]\n",
    "        with open(OUTFILE,'a') as out:\n",
    "            out.write('\\t'.join(map(str,row))+'\\n')\n",
    "        \n",
    "        if epoch_r2 > best_r2:\n",
    "            best_r2 = epoch_r2\n",
    "            best_state = model.state_dict()\n",
    "\n",
    "    ## Test\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    test_true = []\n",
    "    test_pred = []\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader_test:\n",
    "            inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            test_true.append(labels.detach().cpu().numpy())\n",
    "            test_pred.append(outputs.squeeze().detach().cpu().numpy())\n",
    "        test_true = np.concatenate(test_true)\n",
    "        test_pred = np.concatenate(test_pred)\n",
    "\n",
    "    infer_time = (time.time() - start)\n",
    "    val_r2 = best_r2\n",
    "    test_r2 = r2_score(test_true, test_pred)\n",
    "    \n",
    "    mae = '%.4f'%mean_absolute_error(test_true, test_pred)\n",
    "    rmse = '%.4f'%mean_squared_error(test_true, test_pred, squared=False)\n",
    "    row = ['CNN', lr, wd, otherhp, 'Test', 'PBS', 1, '%.4f'%test_r2, mae, rmse, '%.4f'%infer_time]\n",
    "    with open(OUTFILE,'a') as out:\n",
    "        out.write('\\t'.join(map(str,row))+'\\n')\n",
    "    \n",
    "    print(\"CNN; LR %s; WD %s; LD %s\"%(lr,wd,lin_dim))\n",
    "    print(f\"  Validation R2: {val_r2:.4f}\")\n",
    "    print(f\"  Test R2: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_size)\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: (batch, seq_len, hidden_size)\n",
    "        out = self.fc(out[:, -1, :])     # 마지막 time step만 예측에 사용\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [64,128]\n",
    "nums_layers = [1,2]\n",
    "conds = [(lr,wd,hs,nl) for lr in learning_rates for wd in weight_decays \n",
    "         for hs in hidden_sizes for nl in nums_layers]\n",
    "print(len(conds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     7
    ]
   },
   "outputs": [],
   "source": [
    "for lr, wd, hidden_size, num_layers in conds:\n",
    "    model = LSTM(input_size=input_dim, output_size=output_dim, hidden_size=hidden_size, num_layers=num_layers).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd) \n",
    "    best_state = None\n",
    "    best_r2 = float('-inf')\n",
    "    otherhp = 'hidden_size=%i;num_layers=%i'%(hidden_size,num_layers)\n",
    "    ## Training\n",
    "    for epoch in tqdm(range(num_epochs), desc='LR %s; WD %s; HS %s; NL %s, Epoch'%(lr,wd,hidden_size,num_layers)):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        for inputs, labels in loader_train:\n",
    "            inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_time = (time.time() - start)\n",
    "        \n",
    "        model.eval()\n",
    "        cross_loss = 0\n",
    "        cross_true, cross_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in loader_val:\n",
    "                inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                cross_loss += loss.item()\n",
    "                cross_true.append(labels.detach().cpu().numpy())\n",
    "                cross_pred.append(outputs.squeeze().detach().cpu().numpy())\n",
    "            cross_true = np.concatenate(cross_true)\n",
    "            cross_pred = np.concatenate(cross_pred)\n",
    "\n",
    "        epoch_r2 = r2_score(cross_true, cross_pred)\n",
    "        #print('%i\\t%.3f'%(epoch,epoch_r2))\n",
    "        \n",
    "        mae = '%.4f'%mean_absolute_error(cross_true, cross_pred)\n",
    "        rmse = '%.4f'%mean_squared_error(cross_true, cross_pred, squared=False)\n",
    "        row = ['LSTM', lr, wd, otherhp, 'Val', 'PBS', epoch, '%.4f'%epoch_r2, mae, rmse, '%.4f'%train_time]\n",
    "        with open(OUTFILE,'a') as out:\n",
    "            out.write('\\t'.join(map(str,row))+'\\n')\n",
    "        \n",
    "        if epoch_r2 > best_r2:\n",
    "            best_r2 = epoch_r2\n",
    "            best_state = model.state_dict()\n",
    "\n",
    "    ## Test\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    test_true = []\n",
    "    test_pred = []\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader_test:\n",
    "            inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            test_true.append(labels.detach().cpu().numpy())\n",
    "            test_pred.append(outputs.squeeze().detach().cpu().numpy())\n",
    "        test_true = np.concatenate(test_true)\n",
    "        test_pred = np.concatenate(test_pred)\n",
    "\n",
    "    infer_time = (time.time() - start)\n",
    "    val_r2 = best_r2\n",
    "    test_r2 = r2_score(test_true, test_pred)\n",
    "    \n",
    "    mae = '%.4f'%mean_absolute_error(test_true, test_pred)\n",
    "    rmse = '%.4f'%mean_squared_error(test_true, test_pred, squared=False)\n",
    "    row = ['LSTM', lr, wd, otherhp, 'Test', 'PBS', 1, '%.4f'%test_r2, mae, rmse, '%.4f'%infer_time]\n",
    "    with open(OUTFILE,'a') as out:\n",
    "        out.write('\\t'.join(map(str,row))+'\\n')\n",
    "    \n",
    "    print(\"LSTM; LR %s; WD %s; HS %s; NL %s\"%(lr,wd,hidden_size,num_layers))\n",
    "    print(f\"  Validation R2: {val_r2:.4f}\")\n",
    "    print(f\"  Test R2: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, output_dim, num_layers=1, dropout=.1, max_len=1500):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.positional_encoding = self._generate_positional_encoding(model_dim, max_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, \n",
    "                                                   dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_layer = nn.Linear(model_dim, output_dim)\n",
    "\n",
    "    def _generate_positional_encoding(self, d_model, max_len):\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)  # [max_len, 1, model_dim]\n",
    "        return pe  # [seq_len, 1, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # [batch, seq_len, model_dim]\n",
    "        x = x.permute(1, 0, 2)  # [seq_len, batch, model_dim]\n",
    "        x = x + self.positional_encoding[:x.size(0)].to(x.device)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)  # [batch, model_dim]\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dims = [64]\n",
    "nums_heads = [4]\n",
    "\n",
    "conds = [(lr,wd,md,nh) for lr in learning_rates for wd in weight_decays \n",
    "         for md in model_dims for nh in nums_heads]\n",
    "print(len(conds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     7
    ]
   },
   "outputs": [],
   "source": [
    "for lr, wd, model_dim, num_heads in conds:\n",
    "    model = Transformer(input_dim=input_dim, model_dim=model_dim, num_heads=num_heads, \n",
    "                        output_dim=output_dim).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd) \n",
    "    best_state = None\n",
    "    best_r2 = float('-inf')\n",
    "    otherhp = 'model_dim=%i;nheads=%i'%(model_dim,num_heads)\n",
    "    ## Training\n",
    "    for epoch in tqdm(range(num_epochs), \n",
    "                      desc='LR %s; WD %s; MD %s; NH %s; Epoch'%(lr,wd,model_dim,num_heads)):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        for inputs, labels in loader_train:\n",
    "            inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_time = (time.time() - start)\n",
    "        \n",
    "        model.eval()\n",
    "        cross_loss = 0\n",
    "        cross_true, cross_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in loader_val:\n",
    "                inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                cross_loss += loss.item()\n",
    "                cross_true.append(labels.detach().cpu().numpy())\n",
    "                cross_pred.append(outputs.squeeze().detach().cpu().numpy())\n",
    "            cross_true = np.concatenate(cross_true)\n",
    "            cross_pred = np.concatenate(cross_pred)\n",
    "\n",
    "        epoch_r2 = r2_score(cross_true, cross_pred)\n",
    "        #print('%i\\t%.3f'%(epoch,epoch_r2))\n",
    "        \n",
    "        mae = '%.4f'%mean_absolute_error(cross_true, cross_pred)\n",
    "        rmse = '%.4f'%mean_squared_error(cross_true, cross_pred, squared=False)\n",
    "        row = ['Transformer', lr, wd, otherhp, 'Val', 'PBS', epoch, '%.4f'%epoch_r2, mae, rmse, '%.4f'%train_time]\n",
    "        with open(OUTFILE,'a') as out:\n",
    "            out.write('\\t'.join(map(str,row))+'\\n')\n",
    "        \n",
    "        if epoch_r2 > best_r2:\n",
    "            best_r2 = epoch_r2\n",
    "            best_state = model.state_dict()\n",
    "\n",
    "    ## Test\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    test_true = []\n",
    "    test_pred = []\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader_test:\n",
    "            inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            test_true.append(labels.detach().cpu().numpy())\n",
    "            test_pred.append(outputs.squeeze().detach().cpu().numpy())\n",
    "        test_true = np.concatenate(test_true)\n",
    "        test_pred = np.concatenate(test_pred)\n",
    "\n",
    "    infer_time = (time.time() - start)\n",
    "    val_r2 = best_r2\n",
    "    test_r2 = r2_score(test_true, test_pred)\n",
    "    \n",
    "    mae = '%.4f'%mean_absolute_error(test_true, test_pred)\n",
    "    rmse = '%.4f'%mean_squared_error(test_true, test_pred, squared=False)\n",
    "    row = ['Transformer', lr, wd, otherhp, 'Test', 'PBS', 1, '%.4f'%test_r2, mae, rmse, '%.4f'%infer_time]\n",
    "    with open(OUTFILE,'a') as out:\n",
    "        out.write('\\t'.join(map(str,row))+'\\n')\n",
    "\n",
    "    print(\"Transformer; LR %s; WD %s; MD %s; NH %s\"%(lr,wd,model_dim,num_heads))\n",
    "    print(f\"  Validation R2: {val_r2:.4f}\")\n",
    "    print(f\"  Test R2: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_epoch = 20\n",
    "outfile = '/home/jupyter/ADAPT_PCR_share/safe/results/0721_regression_performances.csv'\n",
    "outfile_full = '/home/jupyter/ADAPT_PCR_share/safe/results/0721_regression_performances_full.csv'\n",
    "outcols = ['R2','MAE','RMSE','Training time','Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pbs = pd.read_table(outfile)\n",
    "res_top_pbs = ( results_pbs[(results_pbs['Dataset'] == 'Val')&(results_pbs['Epoch']>=min_epoch)]\n",
    "                .sort_values('R2', ascending=False)\n",
    "                .drop_duplicates(subset=['Architecture', 'LR', 'WD', 'Other_HP'], keep='first') )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = results_pbs[results_pbs['Architecture']=='Transformer']\n",
    "set(sub['Other_HP'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full = pd.read_table(outfile_full)\n",
    "res_top_full = ( results_full[(results_full['Dataset'] == 'Val')&(results_full['Epoch']>=min_epoch)]\n",
    "            .sort_values('R2', ascending=False)\n",
    "            .drop_duplicates(subset=['Architecture', 'LR', 'WD', 'Other_HP'], keep='first') )\n",
    "# top3_per_arch = ( res_top_full\n",
    "#                   .groupby('Architecture', group_keys=False)\n",
    "#                   .apply(lambda df: df.nlargest(3, 'R2')) )\n",
    "# top3_per_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_simple = results[(results['Dataset']=='Test')&(results['Other_HP']=='na')].copy()\n",
    "selected_simple.loc[:,'Input'] = 'Features'\n",
    "selected_simple.loc[:,'Training time'] = 0\n",
    "selected_simple = selected_simple.set_index(['Input','Architecture'])[outcols]\n",
    "selected_simple.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_full = []\n",
    "r_top = res_top_full.drop_duplicates('Architecture').set_index('Architecture')\n",
    "for arch,row in r_top.reindex(['CNN','Transformer','LSTM','Lyra']).iterrows():\n",
    "    lr, wd, other = row.values[:3]\n",
    "    sub_val = results_full[(results_full['Architecture']==arch)&(results_full['Dataset']=='Val')]\n",
    "    sub_test = results_full[(results_full['Architecture']==arch)&(results_full['Dataset']=='Test')]\n",
    "    \n",
    "    select = sub_test[(sub_test['LR']==lr)&(sub_test['WD']==wd)&(sub_test['Other_HP']==other)].copy()\n",
    "    t_train = sub_val.loc[(sub_val['LR']==lr)&(sub_val['WD']==wd)&(sub_val['Other_HP']==other),'Time'].mean()\n",
    "    select.loc[:,'Training time'] = '%.4f'%t_train\n",
    "    select.loc[:,'Input'] = 'Full sequence'\n",
    "    selected_full.append(select)\n",
    "    \n",
    "selected_full = pd.concat(selected_full)\n",
    "selected_full = selected_full.set_index(['Input','Architecture'])[outcols]\n",
    "selected_full.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pbs = []\n",
    "r_top = res_top_pbs.drop_duplicates('Architecture').set_index('Architecture')\n",
    "for arch,row in r_top.reindex(['CNN','Transformer','LSTM','Lyra']).iterrows():\n",
    "    lr, wd, other = row.values[:3]\n",
    "    print(other, lr, wd)\n",
    "    sub_val = results_pbs[(results_pbs['Architecture']==arch)&(results_pbs['Dataset']=='Val')]\n",
    "    sub_test = results_pbs[(results_pbs['Architecture']==arch)&(results_pbs['Dataset']=='Test')]\n",
    "    \n",
    "    select = sub_test[(sub_test['LR']==lr)&(sub_test['WD']==wd)&(sub_test['Other_HP']==other)].copy()\n",
    "    t_train = sub_val.loc[(sub_val['LR']==lr)&(sub_val['WD']==wd)&(sub_val['Other_HP']==other),'Time'].mean()\n",
    "    select.loc[:,'Training time'] = '%.4f'%t_train\n",
    "    select.loc[:,'Input'] = 'PBS only'\n",
    "    selected_pbs.append(select)\n",
    "    \n",
    "selected_pbs = pd.concat(selected_pbs)\n",
    "selected_pbs = selected_pbs.set_index(['Input','Architecture'])[outcols]\n",
    "selected_pbs.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = pd.concat([selected_simple, selected_full, selected_pbs])\n",
    "selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, DL, mlp_dims, dl_dims, combined_hidden, final_output):\n",
    "        super(CombinedModel, self).__init__()\n",
    "\n",
    "        # Individual models\n",
    "        self.mlp = MLP(*mlp_dims)\n",
    "        self.dl = DL(*dl_dims)\n",
    "\n",
    "        # Combining ml\n",
    "        combined_input_dim = mlp_dims[1] + dl_dims[1]\n",
    "        self.combiner = nn.Sequential(\n",
    "            nn.Linear(combined_input_dim, combined_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(combined_hidden, final_output)\n",
    "        )\n",
    "\n",
    "    def forward(self, mlp_input, dl_input):\n",
    "        mlp_out = self.mlp(mlp_input)  # Output from ml\n",
    "        dl_out = self.dl(dl_input)  # Output from dl\n",
    "\n",
    "        # Concatenate outputs\n",
    "        combined = torch.cat((mlp_out, dl_out), dim=1)\n",
    "\n",
    "        # Final prediction\n",
    "        final_output = self.combiner(combined)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PcrDataset(Dataset):\n",
    "    def __init__(self, encoded_input, custom_features, scores):\n",
    "        self.encoded_input = encoded_input\n",
    "        self.custom_features = custom_features\n",
    "        self.scores = scores\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_input)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_input[idx], self.custom_features[idx], self.scores[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "seqs_train = one_hot_encode_pbs_gap(train_df)\n",
    "seqs_val = one_hot_encode_pbs_gap(valid_df)\n",
    "seqs_test = one_hot_encode_pbs_gap(test_df)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feats_train = scaler.fit_transform(train_feats)\n",
    "feats_val = scaler.transform(valid_feats)\n",
    "feats_test = scaler.transform(test_feats)\n",
    "\n",
    "y_train = train_df['score'].values\n",
    "y_val = valid_df['score'].values\n",
    "y_test = test_df['score'].values\n",
    "\n",
    "data_train = PcrDataset(seqs_train, feats_train, y_train)\n",
    "data_val = PcrDataset(seqs_val, feats_val, y_val)\n",
    "data_test = PcrDataset(seqs_test, feats_test, y_test)\n",
    "\n",
    "loader_train = DataLoader(data_train, batch_size=64, shuffle=True)\n",
    "loader_val = DataLoader(data_val, batch_size=64, shuffle=False)\n",
    "loader_test = DataLoader(data_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTFILE = '%s/0721_regression_performances_combined.csv'%SAVEPATH\n",
    "cols = ['Architecture', 'LR', 'WD', 'Other_HP', 'Dataset', 'Encoding', 'Epoch', 'R2', 'MAE', 'RMSE', 'Time']\n",
    "# with open(OUTFILE,'wt') as out:\n",
    "#     out.write('\\t'.join(cols)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lyra + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyra_inp_dim = input_dim\n",
    "lyra_out_dim = 4\n",
    "lyra_mod_dim = [64,128]\n",
    "\n",
    "mlp_inp_dim = train_feats.shape[1]\n",
    "mlp_out_dim = 4\n",
    "mlp_hid_dims = [64,128]\n",
    "\n",
    "com_hid_dims = [16,32]\n",
    "final_out_dim = output_dim\n",
    "\n",
    "lr = .001\n",
    "wds = [0,.01]\n",
    "\n",
    "conds = [(d0,d1,d2,wd) for d0 in lyra_mod_dim for d1 in mlp_hid_dims for d2 in com_hid_dims for wd in wds]\n",
    "print(len(conds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyra_mod_dim, mlp_hid_dim, com_hid_dim, wd in conds:\n",
    "    model = CombinedModel(DL=Janus,\n",
    "                          mlp_dims=(mlp_inp_dim, mlp_out_dim, mlp_hid_dim), \n",
    "                          dl_dims=(lyra_inp_dim, lyra_out_dim, lyra_mod_dim), \n",
    "                          combined_hidden=com_hid_dim, final_output=final_out_dim).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd) \n",
    "    best_state = None\n",
    "    best_r2 = float('-inf')\n",
    "    otherhp = 'lyra_mod_dim=%i;mlp_hid_dim=%i;com_hid_dim=%i'%(lyra_mod_dim,mlp_hid_dim,com_hid_dim)\n",
    "    ## Training\n",
    "    for epoch in tqdm(range(num_epochs), desc='MLP_hidden %s; Comb_hidden %s; WD %s, Epoch'%\\\n",
    "                      (mlp_hid_dim,com_hid_dim,wd)):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        for inputs, mlp_inputs, labels in loader_train:\n",
    "            inputs, mlp_inputs, labels = inputs.to(device).float(), mlp_inputs.to(device).float(), labels.to(device).float()\n",
    "            outputs = model(mlp_inputs, inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_time = (time.time() - start)\n",
    "        \n",
    "        model.eval()\n",
    "        cross_loss = 0\n",
    "        cross_true, cross_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, mlp_inputs, labels in loader_val:\n",
    "                inputs, mlp_inputs, labels = inputs.to(device).float(), mlp_inputs.to(device).float(), labels.to(device).float()\n",
    "                outputs = model(mlp_inputs, inputs)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                cross_loss += loss.item()\n",
    "                cross_true.append(labels.detach().cpu().numpy())\n",
    "                cross_pred.append(outputs.squeeze().detach().cpu().numpy())\n",
    "            cross_true = np.concatenate(cross_true)\n",
    "            cross_pred = np.concatenate(cross_pred)\n",
    "\n",
    "        epoch_r2 = r2_score(cross_true, cross_pred)\n",
    "        #print('%i\\t%.3f'%(epoch,epoch_r2))\n",
    "        \n",
    "        mae = '%.4f'%mean_absolute_error(cross_true, cross_pred)\n",
    "        rmse = '%.4f'%mean_squared_error(cross_true, cross_pred, squared=False)\n",
    "        row = ['Lyra+MLP', lr, wd, otherhp, 'Val', 'PBS', epoch, '%.4f'%epoch_r2, mae, rmse, '%.4f'%train_time]\n",
    "        #with open(OUTFILE,'a') as out:\n",
    "        #    out.write('\\t'.join(map(str,row))+'\\n')\n",
    "        \n",
    "        if epoch_r2 > best_r2:\n",
    "            best_r2 = epoch_r2\n",
    "            best_state = model.state_dict()\n",
    "\n",
    "    ## Test\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    test_true = []\n",
    "    test_pred = []\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for inputs, mlp_inputs, labels in loader_test:\n",
    "            inputs, mlp_inputs, labels = inputs.to(device).float(), mlp_inputs.to(device).float(), labels.to(device).float()\n",
    "            outputs = model(mlp_inputs, inputs)\n",
    "            test_true.append(labels.detach().cpu().numpy())\n",
    "            test_pred.append(outputs.squeeze().detach().cpu().numpy())\n",
    "        test_true = np.concatenate(test_true)\n",
    "        test_pred = np.concatenate(test_pred)\n",
    "\n",
    "    infer_time = (time.time() - start)\n",
    "    val_r2 = best_r2\n",
    "    test_r2 = r2_score(test_true, test_pred)\n",
    "    \n",
    "    mae = '%.4f'%mean_absolute_error(test_true, test_pred)\n",
    "    rmse = '%.4f'%mean_squared_error(test_true, test_pred, squared=False)\n",
    "    row = ['Lyra+MLP', lr, wd, otherhp, 'Test', 'PBS', 1, '%.4f'%test_r2, mae, rmse, '%.4f'%infer_time]\n",
    "    #with open(OUTFILE,'a') as out:\n",
    "    #    out.write('\\t'.join(map(str,row))+'\\n')\n",
    "    mname = ';'.join(map(str,['Lyra+MLP', lr, wd, otherhp]))\n",
    "    modelout = '/home/jupyter/ADAPT_PCR_share/safe/dataset/%s.pth' % mname\n",
    "    torch.save(model, modelout)\n",
    "    print(\"Lyra+MLP; MLP_hidden %s; Comb_hidden %s; WD %s\"%(mlp_hid_dim,com_hid_dim,wd))\n",
    "    print(f\"  Validation R2: {val_r2:.4f}\")\n",
    "    print(f\"  Test R2: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_inp_dim = input_dim\n",
    "lstm_out_dim = 4\n",
    "lstm_hid_dims = [64,128]\n",
    "lstm_nums_layers = [1,4]\n",
    "\n",
    "mlp_inp_dim = train_feats.shape[1]\n",
    "mlp_out_dim = 4\n",
    "mlp_hid_dims = [64,128]\n",
    "\n",
    "com_hid_dims = [16,32]\n",
    "final_out_dim = output_dim\n",
    "\n",
    "lr = .001\n",
    "wds = [0,.01]\n",
    "\n",
    "conds = [(d1,d2,d3,d4,wd) for d1 in mlp_hid_dims for d2 in com_hid_dims \n",
    "         for d3 in lstm_hid_dims for d4 in lstm_nums_layers for wd in wds]\n",
    "print(len(conds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mlp_hid_dim, com_hid_dim, lstm_hid_dim, lstm_num_layers, wd in conds:\n",
    "    model = CombinedModel(DL=LSTM,\n",
    "                          mlp_dims=(mlp_inp_dim, mlp_out_dim, mlp_hid_dim), \n",
    "                          dl_dims=(lstm_inp_dim, lstm_out_dim, lstm_hid_dim, lstm_num_layers), \n",
    "                          combined_hidden=com_hid_dim, final_output=final_out_dim).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd) \n",
    "    best_state = None\n",
    "    best_r2 = float('-inf')\n",
    "    otherhp = 'mlp_hid_dim=%i;com_hid_dim=%i;lstm_hid_dim=%i;lstm_num_layers=%i'%\\\n",
    "                (mlp_hid_dim,com_hid_dim,lstm_hid_dim,lstm_num_layers)\n",
    "    ## Training\n",
    "    for epoch in tqdm(range(num_epochs), desc='MLP_hidden %s; Comb_hidden %s; WD %s; LSTM %s %s, Epoch'%\\\n",
    "                      (mlp_hid_dim,com_hid_dim,wd,lstm_hid_dim,lstm_num_layers)):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        for inputs, mlp_inputs, labels in loader_train:\n",
    "            inputs, mlp_inputs, labels = inputs.to(device).float(), mlp_inputs.to(device).float(), labels.to(device).float()\n",
    "            outputs = model(mlp_inputs, inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_time = (time.time() - start)\n",
    "        \n",
    "        model.eval()\n",
    "        cross_loss = 0\n",
    "        cross_true, cross_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, mlp_inputs, labels in loader_val:\n",
    "                inputs, mlp_inputs, labels = inputs.to(device).float(), mlp_inputs.to(device).float(), labels.to(device).float()\n",
    "                outputs = model(mlp_inputs, inputs)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                cross_loss += loss.item()\n",
    "                cross_true.append(labels.detach().cpu().numpy())\n",
    "                cross_pred.append(outputs.squeeze().detach().cpu().numpy())\n",
    "            cross_true = np.concatenate(cross_true)\n",
    "            cross_pred = np.concatenate(cross_pred)\n",
    "\n",
    "        epoch_r2 = r2_score(cross_true, cross_pred)\n",
    "        #print('%i\\t%.3f'%(epoch,epoch_r2))\n",
    "        \n",
    "        mae = '%.4f'%mean_absolute_error(cross_true, cross_pred)\n",
    "        rmse = '%.4f'%mean_squared_error(cross_true, cross_pred, squared=False)\n",
    "        row = ['LSTM+MLP', lr, wd, otherhp, 'Val', 'PBS', epoch, '%.4f'%epoch_r2, mae, rmse, '%.4f'%train_time]\n",
    "        with open(OUTFILE,'a') as out:\n",
    "            out.write('\\t'.join(map(str,row))+'\\n')\n",
    "        \n",
    "        if epoch_r2 > best_r2:\n",
    "            best_r2 = epoch_r2\n",
    "            best_state = model.state_dict()\n",
    "\n",
    "    ## Test\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    test_true = []\n",
    "    test_pred = []\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for inputs, mlp_inputs, labels in loader_test:\n",
    "            inputs, mlp_inputs, labels = inputs.to(device).float(), mlp_inputs.to(device).float(), labels.to(device).float()\n",
    "            outputs = model(mlp_inputs, inputs)\n",
    "            test_true.append(labels.detach().cpu().numpy())\n",
    "            test_pred.append(outputs.squeeze().detach().cpu().numpy())\n",
    "        test_true = np.concatenate(test_true)\n",
    "        test_pred = np.concatenate(test_pred)\n",
    "\n",
    "    infer_time = (time.time() - start)\n",
    "    val_r2 = best_r2\n",
    "    test_r2 = r2_score(test_true, test_pred)\n",
    "    \n",
    "    mae = '%.4f'%mean_absolute_error(test_true, test_pred)\n",
    "    rmse = '%.4f'%mean_squared_error(test_true, test_pred, squared=False)\n",
    "    row = ['LSTM+MLP', lr, wd, otherhp, 'Test', 'PBS', 1, '%.4f'%test_r2, mae, rmse, '%.4f'%infer_time]\n",
    "    with open(OUTFILE,'a') as out:\n",
    "        out.write('\\t'.join(map(str,row))+'\\n')\n",
    "    \n",
    "    print(\"LSTM+MLP; MLP_hidden %s; Comb_hidden %s; WD %s; LSTM %s %s\"%\\\n",
    "          (mlp_hid_dim,com_hid_dim,wd,lstm_hid_dim,lstm_num_layers))\n",
    "    print(f\"  Validation R2: {val_r2:.4f}\")\n",
    "    print(f\"  Test R2: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = '%s/0721_regression_performances_combined.csv'%SAVEPATH\n",
    "results_com = pd.read_table(outfile)\n",
    "res_top_com = ( results_com[(results_com['Dataset'] == 'Val')&(results_com['Epoch']>=min_epoch)]\n",
    "                .sort_values('R2', ascending=False)\n",
    "                .drop_duplicates(subset=['Architecture', 'LR', 'WD', 'Other_HP'], keep='first') )\n",
    "top3_per_arch = ( res_top_com\n",
    "                  .groupby('Architecture', group_keys=False)\n",
    "                  .apply(lambda df: df.nlargest(3, 'R2')) )\n",
    "top3_per_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_com = []\n",
    "r_top = res_top_com.drop_duplicates('Architecture').set_index('Architecture')\n",
    "for arch,row in r_top.iterrows():\n",
    "    lr, wd, other = row.values[:3]\n",
    "    sub_val = results_com[(results_com['Architecture']==arch)&(results_com['Dataset']=='Val')]\n",
    "    sub_test = results_com[(results_com['Architecture']==arch)&(results_com['Dataset']=='Test')]\n",
    "    \n",
    "    select = sub_test[(sub_test['LR']==lr)&(sub_test['WD']==wd)&(sub_test['Other_HP']==other)].copy()\n",
    "    t_train = sub_val.loc[(sub_val['LR']==lr)&(sub_val['WD']==wd)&(sub_val['Other_HP']==other),'Time'].mean()\n",
    "    select.loc[:,'Training time'] = '%.4f'%t_train\n",
    "    select.loc[:,'Input'] = 'PBS & Features'\n",
    "    selected_com.append(select)\n",
    "    \n",
    "selected_com = pd.concat(selected_com)\n",
    "selected_com = selected_com.set_index(['Input','Architecture'])[outcols]\n",
    "selected_com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_all = pd.concat([selected,selected_com])\n",
    "selected_all.columns = ['R2','MAE','RMSE','Training time','Inferece time']\n",
    "selected_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_all.to_csv(SAVEPATH+'0721_regression_model_selection.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "seqs_train = one_hot_encode_pbs_gap(train_df)\n",
    "seqs_val = one_hot_encode_pbs_gap(valid_df)\n",
    "seqs_test = one_hot_encode_pbs_gap(test_df)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feats_train = scaler.fit_transform(train_feats)\n",
    "feats_val = scaler.transform(valid_feats)\n",
    "feats_test = scaler.transform(test_feats)\n",
    "\n",
    "y_train = train_df['score'].values\n",
    "y_val = valid_df['score'].values\n",
    "y_test = test_df['score'].values\n",
    "\n",
    "data_train = PcrDataset(seqs_train, feats_train, y_train)\n",
    "data_val = PcrDataset(seqs_val, feats_val, y_val)\n",
    "data_test = PcrDataset(seqs_test, feats_test, y_test)\n",
    "\n",
    "loader_train = DataLoader(data_train, batch_size=64, shuffle=True)\n",
    "loader_val = DataLoader(data_val, batch_size=64, shuffle=False)\n",
    "loader_test = DataLoader(data_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = '%s/0721_regression_performances_combined.csv'%SAVEPATH\n",
    "results_com = pd.read_table(outfile)\n",
    "res_top_com = ( results_com[(results_com['Dataset'] == 'Val')&(results_com['Epoch']>=min_epoch)]\n",
    "                .sort_values('R2', ascending=False)\n",
    "                .drop_duplicates(subset=['Architecture', 'LR', 'WD', 'Other_HP'], keep='first') )\n",
    "top3_per_arch = ( res_top_com\n",
    "                  .groupby('Architecture', group_keys=False)\n",
    "                  .apply(lambda df: df.nlargest(3, 'R2')) )\n",
    "top3_per_arch[top3_per_arch['Architecture']=='Lyra+MLP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .001\n",
    "wd = .01\n",
    "\n",
    "lyra_inp_dim = input_dim\n",
    "lyra_out_dim = 4\n",
    "lyra_mod_dim = 64\n",
    "\n",
    "mlp_inp_dim = train_feats.shape[1]\n",
    "mlp_hid_dim = 64\n",
    "mlp_out_dim = 4\n",
    "\n",
    "com_hid_dim = 16\n",
    "\n",
    "final_out_dim = output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CombinedModel(DL=Janus,\n",
    "                      mlp_dims=(mlp_inp_dim, mlp_out_dim, mlp_hid_dim), \n",
    "                      dl_dims=(lyra_inp_dim, lyra_out_dim, lyra_mod_dim), \n",
    "                      combined_hidden=com_hid_dim, final_output=final_out_dim).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd) \n",
    "best_state = None\n",
    "best_r2 = float('-inf')\n",
    "## Training\n",
    "for epoch in tqdm(range(num_epochs), desc='MLP_hidden %s; Comb_hidden %s; WD %s, Epoch'%\\\n",
    "                  (mlp_hid_dim,com_hid_dim,wd)):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    for inputs, mlp_inputs, labels in loader_train:\n",
    "        inputs, mlp_inputs, labels = inputs.to(device).float(), mlp_inputs.to(device).float(), labels.to(device).float()\n",
    "        outputs = model(mlp_inputs, inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_time = (time.time() - start)\n",
    "\n",
    "    model.eval()\n",
    "    cross_loss = 0\n",
    "    cross_true, cross_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, mlp_inputs, labels in loader_val:\n",
    "            inputs, mlp_inputs, labels = inputs.to(device).float(), mlp_inputs.to(device).float(), labels.to(device).float()\n",
    "            outputs = model(mlp_inputs, inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            cross_loss += loss.item()\n",
    "            cross_true.append(labels.detach().cpu().numpy())\n",
    "            cross_pred.append(outputs.squeeze().detach().cpu().numpy())\n",
    "        cross_true = np.concatenate(cross_true)\n",
    "        cross_pred = np.concatenate(cross_pred)\n",
    "\n",
    "    epoch_r2 = r2_score(cross_true, cross_pred)\n",
    "    print('%i\\t%.3f'%(epoch,epoch_r2))\n",
    "\n",
    "    if epoch_r2 > best_r2:\n",
    "        best_r2 = epoch_r2\n",
    "        best_state = model.state_dict()\n",
    "\n",
    "## Test\n",
    "model.load_state_dict(best_state)\n",
    "model.eval()\n",
    "test_true = []\n",
    "test_pred = []\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for inputs, mlp_inputs, labels in loader_test:\n",
    "        inputs, mlp_inputs, labels = inputs.to(device).float(), mlp_inputs.to(device).float(), labels.to(device).float()\n",
    "        outputs = model(mlp_inputs, inputs)\n",
    "        test_true.append(labels.detach().cpu().numpy())\n",
    "        test_pred.append(outputs.squeeze().detach().cpu().numpy())\n",
    "    test_true = np.concatenate(test_true)\n",
    "    test_pred = np.concatenate(test_pred)\n",
    "\n",
    "infer_time = (time.time() - start)\n",
    "val_r2 = best_r2\n",
    "test_r2 = r2_score(test_true, test_pred)\n",
    "\n",
    "print(\"Lyra+MLP; MLP_hidden %s; Comb_hidden %s; WD %s\"%(mlp_hid_dim,com_hid_dim,wd))\n",
    "print(f\"  Validation R2: {val_r2:.4f}\")\n",
    "print(f\"  Test R2: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme = '''\n",
    "Combined model consisting of Janus and MLP\n",
    "- Janus input: 8 x 56 encoding of the sequence;\n",
    "               the upper 4 rows are the one-hot encoding of the target seq, \n",
    "               the lower 4 rows are the one-hot encoding of the primer seq.\n",
    "- MLP input: 12 features scaled by standard scaling.\n",
    "- Output: Score (0-1) for each primer pair and target sequence.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBPATH = '/home/jupyter/ADAPT_PCR_share/safe/dataset/0725_ml_combined_12feat'\n",
    "if not os.path.exists(SUBPATH):\n",
    "    os.makedirs(SUBPATH)\n",
    "    \n",
    "modelout = '%s/model.pth'%SUBPATH\n",
    "scaleout = '%s/scaler.joblib'%SUBPATH\n",
    "readmeout = '%s/README'%SUBPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "scaleout = '/home/jupyter/ADAPT_PCR_share/safe/design/pipeline/0728_scaler.joblib'\n",
    "#torch.save(model, modelout)\n",
    "dump(scaler, scaleout)\n",
    "# with open(readmeout,'wt') as out:\n",
    "#     out.write(readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(scaler, \"scaler.joblib\")\n",
    "# 불러오기\n",
    "scaler = joblib.load(\"scaler.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
